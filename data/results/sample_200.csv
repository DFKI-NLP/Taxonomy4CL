acl_id,Level1,Level2,Level3,abstract,url,publisher,year,month,booktitle,author,title,doi,venue
2020.dmr-1.8,"['Parsing', 'Knowledge Representation and Reasoning']",['Semantic Parsing'],,"We propose an approach and a software framework for semantic parsing of natural language sentences to discourse representation structures with use of fuzzy meaning representations such as fuzzy sets and compatibility intervals. We explain the motivation for using fuzzy meaning representations in semantic parsing and describe the design of the proposed approach and the software framework, discussing various examples. We argue that the use of fuzzy meaning representations have potential to improve understanding and reasoning capabilities of systems working with natural language.",https://aclanthology.org/2020.dmr-1.8,Association for Computational Linguistics,2020,December,Proceedings of the Second International Workshop on Designing Meaning Representations,"Kapustin, Pavlo  and
Kapustin, Michael",Semantic parsing with fuzzy meaning representations,,dmr
2020.vlsp-1.6,"['Learning Paradigms', 'Information Extraction', 'Model Architectures', 'Low-resource Languages']","['Relation Extraction', 'Supervised Learning', 'Transformer Models']",,"In recent years, BERT-based models have achieved the state-of-the-art performance over many Natural Language Language tasks. Because of that, BERT-based model becomes a trend and is widely used for so many NLP task. And in this paper, we present our approach on how we apply BERT-based model to Relation Extraction shared-task of VLSP 2020 campaign. In detail, we present: 1 our general idea to solve this task; 2 how we preprocess data to fit with the idea and to yield better result; 3 how we use BERT-based models for Relation Extraction task; and 4 our experiment and result on public development data and private test data of VLSP 2020.",https://aclanthology.org/2020.vlsp-1.6,Association for Computational Lingustics,2020,December,Proceedings of the 7th International Workshop on Vietnamese Language and Speech Processing,"Nguyễn, Thuật  and
Mẫn, Hiếu",Vietnamese Relation Extraction with BERT-based Models at VLSP 2020,,vlsp
2020.globalex-1.5,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],,"This extended abstract presents on-going work consisting in interlinking and merging the Open Dutch WordNet and generic lexicographic resources for Dutch, focusing for now on the Dutch and English versions of Wiktionary and using the Algemeen Nederlands Woordenboek as a quality checking instance. As the Open Dutch WordNet is already equipped with a relevant number of complex lexical units, we are aiming at expanding it and proposing a new representational framework for the encoding of the interlinked and integrated data. The longer term goal of the work is to investigate if and on how senses can be restricted to particular morphological variations of Dutch lexical entries, and how to represent this information in a Linguistic Linked Open Data compliant format.",https://aclanthology.org/2020.globalex-1.5,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Declerck, Thierry",Towards an Extension of the Linking of the Open Dutch WordNet with Dutch Lexicographic Resources,,globalex
2021.bsnlp-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"Abusive phenomena are commonplace in language on the web. The scope of recognizing abusive language is broad, covering many behaviours and forms of expression. This work addresses automatic detection of abusive language in Russian. The lexical, grammatical and morphological diversity of Russian language present potential difficulties for this task, which is addressed using a variety of machine learning approaches. We present a dataset and baselines for this task.",https://aclanthology.org/2021.bsnlp-1.3,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Saitov, Kamil  and
Derczynski, Leon",Abusive Language Recognition in Russian,,bsnlp
2020.lrec-1.758,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection']","['NLP for Social Media', 'Annotation Processes']","This paper introduces a corpus of Turkish offensive language. To our knowledge, this is the first corpus of offensive language for Turkish. The corpus consists of randomly sampled micro-blog posts from Twitter. The annotation guidelines are based on a careful review of the annotation practices of recent efforts for other languages. The corpus contains 36 232 tweets sampled randomly from the Twitter stream during a period of 18 months between Apr 2018 to Sept 2019. We found approximately 19 % of the tweets in the data contain some type of offensive language, which is further subcategorized based on the target of the offense. We describe the annotation process, discuss some interesting aspects of the data, and present results of automatically classifying the corpus using state-of-the-art text classification methods. The classifiers achieve 77.3 % F1 score on identifying offensive tweets, 77.9 % F1 score on determining whether a given offensive document is targeted or not, and 53.0 % F1 score on classifying the targeted offensive documents into three subcategories.",https://aclanthology.org/2020.lrec-1.758,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"{\c{C}}{\""o}ltekin, {\c{C}}a{\u{g}}r{\i}",A Corpus of Turkish Offensive Language on Social Media,,lrec
2021.blackboxnlp-1.36,"['Domain-specific NLP', 'Data Management and Generation', 'Embeddings', 'Adversarial Attacks and Robustness']","['Data Preparation', 'Data Augmentation']",,"Recently, some studies have shown that text classification tasks are vulnerable to poisoning and evasion attacks. However, little work has investigated attacks against decision-making algorithms that use text embeddings, and their output is a ranking. In this paper, we focus on ranking algorithms for the recruitment process that employ text embeddings for ranking applicants' resumes when compared to a job description. We demonstrate both white-box and black-box attacks that identify text items that, based on their location in embedding space, have a significant contribution in increasing the similarity score between a resume and a job description. The adversary then uses these text items to improve the ranking of their resume among others. We tested recruitment algorithms that use the similarity scores obtained from Universal Sentence Encoder USE and Term Frequency-Inverse Document Frequency TF-IDF vectors. Our results show that in both adversarial settings, on average the attacker is successful. We also found that attacks against TF-IDF are more successful compared to USE.",https://aclanthology.org/2021.blackboxnlp-1.36,Association for Computational Linguistics,2021,November,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Samadi, Anahita  and
Banerjee, Debapriya  and
Nilizadeh, Shirin",Attacks against Ranking Algorithms with Text Embeddings: A Case Study on Recruitment Algorithms,10.18653/v1/2021.blackboxnlp-1.36,blackboxnlp
2020.ccl-1.91,"['Learning Paradigms', 'Low-resource Languages', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Reinforcement Learning']",,"Reinforcement learning (RL) has made remarkable progress in neural machine translation (N-MT). However, it exists the problems with uneven sampling distribution, sparse rewards and high variance in training phase. Therefore, we propose a multi-reward reinforcement learning training strategy to decouple action selection and value estimation. Meanwhile, our method combines with language model rewards to jointly optimize model parameters. In addition, we add Gumbel noise in sampling to obtain more effective semantic information. To verify the robustness of our method, we not only conducted experiments on large corpora, but also performed on low-resource languages. Experimental results show that our work is superior to the baselines in WMT14 English-German, LDC2014 Chinese-English and CWMT2018 Mongolian-Chinese tasks, which fully certificates the effectiveness of our method.",https://aclanthology.org/2020.ccl-1.91,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Sun, Shuo  and
Hou, Hongxu  and
Wu, Nier  and
Guo, Ziyue  and
Zhang, Chaowei",Multi-Reward based Reinforcement Learning for Neural Machine Translation,,ccl
2016.iwslt-1.11,"['Parsing', 'Low-resource Languages', 'Machine Translation (MT)']",['Syntactic Parsing'],,"String-to-tree MT systems translate verbs without lexical or syntactic context on the source side and with limited targetside context. The lack of context is one reason why verb translation recall is as low as 45.5%. We propose a verb lexicon model trained with a feedforward neural network that predicts the target verb conditioned on a wide source-side context. We show that a syntactic context extracted from the dependency parse of the source sentence improves the model's accuracy by 1.5% over a baseline trained on a window context. When used as an extra feature for re-ranking the n-best list produced by the string-to-tree MT system, the verb lexicon model improves verb translation recall by more than 7%.",https://aclanthology.org/2016.iwslt-1.11,International Workshop on Spoken Language Translation,2016,December 8-9,Proceedings of the 13th International Conference on Spoken Language Translation,"N{\u{a}}dejde, Maria  and
Birch, Alexandra  and
Koehn, Philipp",A Neural Verb Lexicon Model with Source-side Syntactic Context for String-to-Tree Machine Translation,,iwslt
2020.udw-1.4,"['Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"We use Universal Dependencies treebanks to test whether a well-known typological trade-off between word order freedom and richness of morphological marking of core arguments holds within individual languages. Using Russian and German treebank data, we show that the following phenomenon sometimes dubbed word order freezing does occur: those sentences where core arguments cannot be distinguished by morphological means due to case syncretism or other kinds of ambiguity have more rigid order of subject, verb and object than those where unambiguous morphological marking is present. In ambiguous clauses, word order is more often equal to the one which is default or dominant most frequent in the language. While Russian and German differ with respect to how exactly they mark core arguments, the effect of morphological ambiguity is significant in both languages. It is, however, small, suggesting that languages do adapt to the evolutionary pressure on communicative efficiency and avoidance of redundancy, but that the pressure is weak in this particular respect.",https://aclanthology.org/2020.udw-1.4,Association for Computational Linguistics,2020,December,Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),"Berdicevskis, Aleksandrs  and
Piperski, Alexander",Corpus evidence for word order freezing in Russian and German,,udw
2020.findings-emnlp.59,"['Model Architectures', 'Classification Applications']",,,"Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information word embedding with positional or temporal information word orders. In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network CSPAN in the context of document classification. The CSPAN uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residual connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between semantics and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high convergence rate. We evaluate the CSPAN model on several benchmark data sets for document classification with careful ablation studies, and demonstrate the encouraging results compared with state of the art.",https://aclanthology.org/2020.findings-emnlp.59,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Jiang, Juyong  and
Zhang, Jie  and
Zhang, Kai",Cascaded Semantic and Positional Self-Attention Network for Document Classification,10.18653/v1/2020.findings-emnlp.59,findings
2020.rocling-1.4,"['Text Preprocessing', 'Information Extraction', 'Model Architectures', 'Low-resource Languages']",['Text Segmentation'],['Word Segmentation'],"The prevalence of the web has brought about the construction of many large-scale, automatically segmented and tagged corpora, which inevitably introduces errors due to automation and are likely to have negative impacts on downstream tasks. Collocation extraction from Chinese corpora is one such task that is profoundly influenced by the quality of word segmentation. This paper explores methods to mitigate the negative impacts of word segmentation errors on collocation extraction in Chinese. In particular, we experimented with a simple model that aims to combine several association measures linearly to avoid retrieving false collocations resulting from word segmentation errors. The results of the experiment show that this simple model could not differentiate between true collocations and false collocations",https://aclanthology.org/2020.rocling-1.4,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2020,September,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),"Liao, Yongfu  and
Hsieh, Shu-Kai",Mitigating Impacts of Word Segmentation Errors on Collocation Extraction in {C}hinese,,rocling
2021.cinlp-1.4,"['Domain-specific NLP', 'Classification Applications']",,,"Despite peer-reviewing being an essential component of academia since the 1600s, it has repeatedly received criticisms for lack of transparency and consistency. We posit that recent work in machine learning and explainable AI provide tools that enable insights into the decisions from a given peer-review process. We start by simulating the peer-review process using an ML classifier and extracting global explanations in the form of linguistic features that affect the acceptance of a scientific paper for publication on an open peerreview dataset. Second, since such global explanations do not justify causal interpretations, we propose a methodology for detecting confounding effects in natural language and generating explanations, disentangled from textual confounders, in the form of lexicons. Our proposed linguistic explanation methodology indicates the following on a case dataset of ICLR submissions: a the organising committee follows, for the most part, the recommendations of reviewers, and b the paper's main characteristics that led to reviewers recommending acceptance for publication are originality, clarity and substance.",https://aclanthology.org/2021.cinlp-1.4,Association for Computational Linguistics,2021,November,Proceedings of the First Workshop on Causal Inference and NLP,"Fytas, Panagiotis  and
Rizos, Georgios  and
Specia, Lucia",What Makes a Scientific Paper be Accepted for Publication?,10.18653/v1/2021.cinlp-1.4,cinlp
2020.aacl-main.45,"['Learning Paradigms', 'Model Architectures', 'Machine Translation (MT)']",['Neural MT (NMT)'],,"Pairwise data automatically constructed from weakly supervised signals has been widely used for training deep learning models. Pairwise datasets such as parallel texts can have uneven quality levels overall, but usually contain data subsets that are more useful as learning examples. We present two methods to refine data that are aimed at obtaining that kind of subsets in a self-supervised way. Our methods are based on iteratively training dualencoder models to compute similarity scores. We evaluate our methods on de-noising parallel texts and training neural machine translation models. We find that: i The self-supervised refinement achieves most machine translation gains in the first iteration, but following iterations further improve its intrinsic evaluation. ii Machine translations can improve the de-noising performance when combined with selection steps. iii Our methods are able to reach the performance of a supervised method. Being entirely self-supervised, our methods are well-suited to handle pairwise data without the need of prior knowledge or human annotations.",https://aclanthology.org/2020.aacl-main.45,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Hernandez Abrego, Gustavo  and
Liang, Bowen  and
Wang, Wei  and
Parekh, Zarana  and
Yang, Yinfei  and
Sung, Yunhsuan",Self-Supervised Learning for Pairwise Data Refinement,,aacl
2020.cmlc-1.4,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation']","['Named Entity Recognition (NER)', 'Data Analysis']",,"This paper describes work in progress on devising automatic and parallel methods for geoparsing large digital historical textual data by combining the strengths of three natural language processing NLP tools, the Edinburgh Geoparser, spaCy and defoe, and employing different tokenisation and named entity recognition NER techniques. We apply these tools to a large collection of nineteenth century Scottish geographical dictionaries, and describe preliminary results obtained when processing this data.",https://aclanthology.org/2020.cmlc-1.4,European Language Ressources Association,2020,May,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,"Filgueira, Rosa  and
Grover, Claire  and
Terras, Melissa  and
Alex, Beatrice",Geoparsing the historical Gazetteers of Scotland: accurately computing location in mass digitised texts,,cmlc
2020.alta-1.10,"['Data Management and Generation', 'Model Architectures', 'Classification Applications']","['Data Analysis', 'Transformer Models']",,"Free text data from social media is now widely used in natural language processing research, and one of the most common machine learning tasks performed on this data is classification. Generally speaking, performances of supervised classification algorithms on social media datasets are lower than those on texts from other sources, but recentlyproposed transformer-based models have considerably improved upon legacy state-of-theart systems. Currently, there is no study that compares the performances of different variants of transformer-based models on a wide range of social media text classification datasets. In this paper, we benchmark the performances of transformer-based pre-trained models on 25 social media text classification datasets, 6 of which are health-related. We compare three pre-trained language models, RoBERTa-base, BERTweet and Clinical-BioBERT in terms of classification accuracy. Our experiments show that RoBERTa-base and BERTweet perform comparably on most datasets, and considerably better than Clinical-BioBERT, even on health-related datasets.",https://aclanthology.org/2020.alta-1.10,Australasian Language Technology Association,2020,December,Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,"Guo, Yuting  and
Dong, Xiangjue  and
Al-Garadi, Mohammed Ali  and
Sarker, Abeed  and
Paris, Cecile  and
Aliod, Diego Moll{\'a}",Benchmarking of Transformer-Based Pre-Trained Models on Social Media Text Classification Datasets,,alta
2020.nli-1.5,"['Text Preprocessing', 'Data Management and Generation', 'Model Architectures', 'Text Generation']","['Data Preparation', 'Transformer Models']",,"Text normalization and sanitization are intrinsic components of Natural Language Inferences. In Information Retrieval or Dialogue Generation, normalization of user queries or utterances enhances linguistic understanding by translating non-canonical text to its canonical form, on which many state-of-the-art language models are trained. On the other hand, text sanitization removes sensitive information to guarantee user privacy and anonymity. Existing approaches to normalization and sanitization mainly rely on hand-crafted heuristics and syntactic features of individual tokens while disregarding the linguistic context. Moreover, such context-unaware solutions cannot dynamically determine whether out-of-vocab tokens are misspelt or are entity names. In this work, we formulate text normalization and sanitization as a multi-task text generation approach and propose a neural pointer-generator network based on multihead attention. Its generator effectively captures linguistic context during normalization and sanitization while its pointer dynamically preserves the entities that are generally missing in the vocabulary. Experiments show that our generation approach outperforms both token-based text normalization and sanitization, while the pointer-generator improves the generator-only baseline in terms of BLEU4 score, and classical attentional pointer networks in terms of pointing accuracy.",https://aclanthology.org/2020.nli-1.5,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Interfaces,"Nguyen, Hoang  and
Cavallari, Sandro",Neural Multi-task Text Normalization and Sanitization with Pointer-Generator,10.18653/v1/2020.nli-1.5,nli
2020.ai4hi-1.1,"['Data Management and Generation', 'Image and Video Processing', 'Knowledge Representation and Reasoning']","['Image Captioning', 'Ontologies']",,"Cultural institutions such as galleries, libraries, archives and museums continue to make commitments to large scale digitization of collections. An ongoing challenge is how to increase discovery and access through structured data and the semantic web. In this paper we describe a method for using computer vision algorithms that automatically detect regions of ""stuff""-such as the sky, water, and roads-to produce rich and accurate structured data triples for describing the content of historic photography. We apply our method to a collection of 1610 documentary photographs produced in the 1930s and 1940 by the FSA-OWI division of the U.S. federal government. Manual verification of the extracted annotations yields an accuracy rate of 97.5%, compared to 70.7% for relations extracted from object detection and 31.5% for automatically generated captions. Our method also produces a rich set of features, providing more unique labels 1170 than either the captions 1040 or object detection 178 methods. We conclude by describing directions for a linguistically-focused ontology of region categories that can better enrich historical image data. Open source code and the extracted metadata from our corpus are made available as external resources.",https://aclanthology.org/2020.ai4hi-1.1,European Language Resources Association (ELRA),2020,May,Proceedings of the 1st International Workshop on Artificial Intelligence for Historical Image Enrichment and Access,"Arnold, Taylor  and
Tilton, Lauren",Enriching Historic Photography with Structured Data using Image Region Segmentation,,ai4hi
2020.alvr-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Text Generation', 'Image and Video Processing', 'Model Architectures']","['Supervised Learning', 'Data Augmentation', 'Medical and Clinical NLP']",,"Visual Question Generation VQG, the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at https://github.com/ sarrouti/vqgr.",https://aclanthology.org/2020.alvr-1.3,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Advances in Language and Vision Research,"Sarrouti, Mourad  and
Ben Abacha, Asma  and
Demner-Fushman, Dina",Visual Question Generation from Radiology Images,10.18653/v1/2020.alvr-1.3,alvr
2021.bsnlp-1.2,"['Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Data Preparation', 'Data Augmentation', 'Paraphrase and Rephrase Generation']",,"This paper focuses on generation methods for paraphrasing in the Russian language. There are several transformer-based models Russian and multilingual trained on a collected corpus of paraphrases. We compare different models, contrast the quality of paraphrases using different ranking methods and apply paraphrasing methods in the context of augmentation procedure for different tasks. The contributions of the work are the combined paraphrasing dataset, fine-tuned generated models for Russian paraphrasing task and additionally the open source tool for simple usage of the paraphrasers.",https://aclanthology.org/2021.bsnlp-1.2,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Fenogenova, Alena",Russian Paraphrasers: Paraphrase with Transformers,,bsnlp
2020.textgraphs-1.1,['Knowledge Representation and Reasoning'],"['Knowledge Graphs', 'Link Prediction']",,"Knowledge graphs KGs of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.",https://aclanthology.org/2020.textgraphs-1.1,Association for Computational Linguistics,2020,December,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),"Nguyen, Dat Quoc",A survey of embedding models of entities and relationships for knowledge graph completion,10.18653/v1/2020.textgraphs-1.1,textgraphs
2020.eamt-1.36,['Machine Translation (MT)'],,,This paper presents a case study of applying machine translation quality estimation QE for the purpose of machine translation MT engine selection. The goal is to understand how well the QE predictions correlate with several MT evaluation metrics automatic and human. Our findings show that our industry-level QE system is not reliable enough for MT selection when the MT systems have similar performance. We suggest that QE can be used with more success for other tasks relevant for translation industry such as risk prevention.,https://aclanthology.org/2020.eamt-1.36,European Association for Machine Translation,2020,November,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,"Zaretskaya, Anna  and
Concei{\c{c}}{\~a}o, Jos{\'e}  and
Bane, Frederick",Estimation vs Metrics: is QE Useful for MT Model Selection?,,eamt
2020.autosimtrans-1.6,"['Data Management and Generation', 'Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures', 'Text Preprocessing', 'Machine Translation (MT)']","['Data Augmentation', 'Transfer Learning', 'Text Segmentation', 'Neural MT (NMT)', 'Automatic Speech Recognition (ASR)']",['Sentence Segmentation'],This paper describes our machine translation systems for the streaming Chinese-to-English translation task of AutoSimTrans 2020. We present a sentence length based method and a sentence boundary detection model based method for the streaming input segmentation. Experimental results of the transcription and the ASR output translation on the development data sets show that the translation system with the detection model based method outperforms the one with the length based method in BLEU score by 1.19 and 0.99 respectively under similar or better latency.,https://aclanthology.org/2020.autosimtrans-1.6,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Automatic Simultaneous Translation,"Li, Minqin  and
Cheng, Haodong  and
Wang, Yuanjie  and
Zhang, Sijia  and
Wu, Liting  and
Guo, Yuhang",BIT's system for the AutoSimTrans 2020,10.18653/v1/2020.autosimtrans-1.6,autosimtrans
2020.osact-1.12,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"The abusive content on Arabic social media such as hate speech, sexism, racism has become pervasive, and it has a lot of negative psychological effects on users. In this paper, we introduce our work aiming to detect Arabic offensive language and hate speech. We present our two deep neural networks Convolutional Neural Network CNN and Bidirectional Gated Recurrent Unit Bi-GRU used to tackle this problem. These models have been further augmented with attention layers. In addition, we have tested various pre-processing and oversampling techniques to increase the performance of our models. Several machine learning algorithms with different features have been also tested. Our bidirectional GRU model augmented with attention layer has achieved the highest results among our proposed models on a labeled dataset of Arabic tweets, where we achieved 0.859 F1 score for the task of offensive language detection, and 0.75 F1 score for the task of hate speech detection.",https://aclanthology.org/2020.osact-1.12,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Haddad, Bushr  and
Orabe, Zoher  and
Al-Abood, Anas  and
Ghneim, Nada",Arabic Offensive Language Detection with Attention-based Deep Neural Networks,,osact
2020.lrec-1.161,"['Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['Data Preparation', 'Misinformation Detection', 'Data Analysis']",,"Fact-checking information before publication has long been a core task for journalists, but recent times have seen the emergence of dedicated news items specifically aimed at fact-checking after publication. This relatively new form of fact-checking receives a fair amount of attention from academics, with current research focusing mostly on journalists' motivations for publishing post-hoc factchecks, the effects of fact-checking on the perceived accuracy of false claims, and the creation of computational tools for automatic factchecking. In this paper, we propose to study fact-checks from a corpus linguistic perspective. This will enable us to gain insight in the scope and contents of fact-checks, to investigate what fact-checks can teach us about the way in which science appears incorrectly in the news, and to see how fact-checks behave in the science communication landscape. We report on the creation of FactCorp, a 1,16 million-word corpus containing 1,974 fact-checks from three major Dutch newspapers. We also present results of several exploratory analyses, including a rhetorical moves analysis, a qualitative content elements analysis, and keyword analyses. Through these analyses, we aim to demonstrate the wealth of possible applications that FactCorp allows, thereby stressing the importance of creating such resources.",https://aclanthology.org/2020.lrec-1.161,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"van der Meulen, Marten  and
Reijnierse, W. Gudrun",FactCorp: A Corpus of Dutch Fact-checks and its Multiple Usages,,lrec
2020.alw-1.9,"['Biases in NLP', 'Data Management and Generation', 'Classification Applications']","['Hate and Offensive Speech Detection', 'Data Analysis']",,"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al.  2019  to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.",https://aclanthology.org/2020.alw-1.9,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Razo, Dante  and
K{\""u}bler, Sandra",Investigating Sampling Bias in Abusive Language Detection,10.18653/v1/2020.alw-1.9,alw
2020.argmining-1.12,"['Data Management and Generation', 'Classification Applications']","['Data Preparation', 'Data Analysis', 'Multilabel Text Classification']",,"Using the appropriate style is key for writing a high-quality text. Reliable computational style analysis is hence essential for the automation of nearly all kinds of text synthesis tasks. Research on style analysis focuses on recognition problems such as authorship identification; the respective technology e.g., n-gram distribution divergence quantification showed to be effective for discrimination, but inappropriate for text synthesis since the ""essence of a style"" remains implicit. This paper contributes right here: it studies the automatic analysis of style at the knowledge-level based on rhetorical devices. To this end, we developed and evaluated a grammar-based approach for identifying 26 syntax-based devices. Then, we employed that approach to distinguish various patterns of style in selected sets of argumentative articles and presidential debates. The patterns reveal several insights into the style used there, while being adequate for integration in text synthesis systems.",https://aclanthology.org/2020.argmining-1.12,Association for Computational Linguistics,2020,December,Proceedings of the 7th Workshop on Argument Mining,"Al Khatib, Khalid  and
Morari, Viorel  and
Stein, Benno",Style Analysis of Argumentative Texts by Mining Rhetorical Devices,,argmining
2021.calcs-1.2,"['Evaluation Techniques', 'Multilingual NLP', 'Low-resource Languages']",,,"Code-mixing is a frequent communication style among multilingual speakers where they mix words and phrases from two different languages in the same utterance of text or speech. Identifying and filtering code-mixed text is a challenging task due to its co-existence with monolingual and noisy text. Over the years, several code-mixing metrics have been extensively used to identify and validate codemixed text quality. This paper demonstrates several inherent limitations of code-mixing metrics with examples from the already existing datasets that are popularly used across various experiments.",https://aclanthology.org/2021.calcs-1.2,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,"Srivastava, Vivek  and
Singh, Mayank",Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text,10.18653/v1/2021.calcs-1.2,calcs
2020.aacl-main.90,"['Data Management and Generation', 'Question Answering (QA)', 'Low-resource Languages', 'Learning Paradigms', 'Cross-lingual Application', 'Multilingual NLP', 'Model Architectures']","['Visual QA (VQA)', 'Data Preparation', 'Multimodal Learning']",,"In this paper, we propose an effective deep learning framework for multilingual and codemixed visual question answering. The proposed model is capable of predicting answers from the questions in Hindi, English or Codemixed Hinglish: Hindi-English languages. The majority of the existing techniques on Visual Question Answering VQA focus on English questions only. However, many applications such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust technique capable of handling the multilingual and code-mixed question to provide the answer against the visual information image. To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the different languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image features. We perform extensive evaluation and ablation studies for English, Hindi and Codemixed VQA. The evaluation shows that the proposed multilingual model achieves state-ofthe-art performance in all these settings.",https://aclanthology.org/2020.aacl-main.90,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Gupta, Deepak  and
Lenka, Pabitra  and
Ekbal, Asif  and
Bhattacharyya, Pushpak",A Unified Framework for Multilingual and Code-Mixed Visual Question Answering,,aacl
2019.icon-1.21,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"This paper proposes a metric to quantify lexical complexity in Malayalam. The metric utilizes word frequency, orthography and morphology as the three factors affecting visual word recognition in Malayalam. Malayalam differs from other Indian languages due to its agglutinative morphology and orthography, which are incorporated into our model. The predictions made by our model are then evaluated against reaction times in a lexical decision task. We find that reaction times are predicted by frequency, morphological complexity and script complexity. We also explore the interactions between morphological complexity with frequency and script in our results. To the best of our knowledge, this is the first study on lexical complexity in Malayalam.",https://aclanthology.org/2019.icon-1.21,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Shallam, Richard  and
Vaidya, Ashwini",Towards measuring lexical complexity in Malayalam,,icon
2020.sustainlp-1.3,"['Domain-specific NLP', 'Information Extraction', 'Model Architectures', 'Learning Paradigms']","['Relation Extraction', 'Supervised Learning', 'Medical and Clinical NLP', 'Unsupervised Learning']",['Biomedical NLP'],"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence mention-level or across an entire corpus pair-level. In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.",https://aclanthology.org/2020.sustainlp-1.3,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Shah, Harshil  and
Fauqueur, Julien",Learning Informative Representations of Biomedical Relations with Latent Variable Models,10.18653/v1/2020.sustainlp-1.3,sustainlp
2021.argmining-1.20,"['Argument Mining', 'Classification Applications', 'Automatic Text Summarization']",,,"Key point analysis KPA has been proposed as a way to summarize arguments with shortsized pieces of text termed key points. This work aims at describing a solution for the Track 1 of the KPA 2021 shared task, analyzing different methodologies for the specific problem of key point matching, which consists in finding a reasonable mapping from arguments to key points. The analysis will focus on transformer based architectures, experimentally investigating the effectiveness of variants specifically tailored to the task.",https://aclanthology.org/2021.argmining-1.20,Association for Computational Linguistics,2021,November,Proceedings of the 8th Workshop on Argument Mining,"Cosenza, Emanuele",Key Point Matching with Transformers,10.18653/v1/2021.argmining-1.20,argmining
2020.ccl-1.79,"['Image and Video Processing', 'Information Extraction', 'Model Architectures']",['Graph Neural Networks (GNNs)'],,"Information extraction from documents such as receipts or invoices is a fundamental and crucial step for office automation. Many approaches focus on extracting entities and relationships from plain texts, however, when it comes to document images, such demand becomes quite challenging since visual and layout information are also of great significance to help tackle this problem. In this work, we propose the attention-based graph neural network to combine textual and visual information from document images. Moreover, the global node is introduced in our graph construction algorithm which is used as a virtual hub to collect the information from all the nodes and edges to help improve the performance. Extensive experiments on real-world datasets show that our method outperforms baseline methods by significant margins.",https://aclanthology.org/2020.ccl-1.79,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Hua, Yuan  and
Huang, Zheng  and
Guo, Jie  and
Qiu, Weidong",Attention-Based Graph Neural Network with Global Context Awareness for Document Understanding,,ccl
2016.eamt-2.18,"['Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",,,"MTExpert is AMPLEXOR's proprietary machine translation MT system based on state-of-the-art statistical and linguistic algorithms, easily integrated with existing linguistic assets, delivering quality results tailored to different communication objectives.",https://aclanthology.org/2016.eamt-2.18,Baltic Journal of Modern Computing,2016,May 30{--}June 1,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,"Ceausu, Alexandru  and
Hunsicker, Sabine  and
Droumaguet, Tudy",Amplexor MTExpert -- machine translation adapted to the translation workflow,,eamt
2020.emnlp-main.712,['Question Answering (QA)'],,,"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments 1 suggest that there hasn't been much progress in multifact QA in the reading comprehension setting. For a recent large-scale model XLNet, we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning 19 points in answer F1. It is complementary to adversarial approaches, yielding further reductions in conjunction.",https://aclanthology.org/2020.emnlp-main.712,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Trivedi, Harsh  and
Balasubramanian, Niranjan  and
Khot, Tushar  and
Sabharwal, Ashish",Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning,10.18653/v1/2020.emnlp-main.712,emnlp
2020.ngt-1.3,"['Learning Paradigms', 'Model Architectures', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Supervised Learning', 'Transformer Models']",,"We propose a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In training an encoder-decoder model, typically, the output of the last layer of the N -layer encoder is fed to the M -layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of N × M losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes N × M models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. Given our flexible tied model, we also address to a-priori selection of the number of encoder and decoder layers for faster decoding, and explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality.",https://aclanthology.org/2020.ngt-1.3,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Dabre, Raj  and
Rubino, Raphael  and
Fujita, Atsushi",Balancing Cost and Benefit with Tied-Multi Transformers,10.18653/v1/2020.ngt-1.3,ngt
2020.nlpcovid19-acl.17,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Transfer Learning', 'Medical and Clinical NLP', 'Misinformation Detection', 'Multilabel Text Classification', 'Data Preparation']",,"We present a simple NLP methodology for detecting COVID-19 misinformation videos on YouTube by leveraging user comments. We use transfer learning pre-trained models to generate a multi-label classifier that can categorize conspiratorial content. We use the percentage of misinformation comments on each video as a new feature for video classification. We show that the inclusion of this feature in simple models yields an accuracy of up to 82.2%. Furthermore, we verify the significance of the feature by performing a Bayesian analysis. Finally, we show that adding the first hundred comments as tf-idf features increases the video classifier accuracy by up to 89.4%.",https://aclanthology.org/2020.nlpcovid19-acl.17,Association for Computational Linguistics,2020,July,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,"Medina Serrano, Juan Carlos  and
Papakyriakopoulos, Orestis  and
Hegelich, Simon",NLP-based Feature Extraction for the Detection of COVID-19 Misinformation Videos on YouTube,,nlpcovid19
2021.bionlp-1.2,"['Domain-specific NLP', 'Information Extraction']","['Medical and Clinical NLP', 'Entity Linking']",['Biomedical NLP'],"Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet mining makes training efficient even with hundreds of thousands of concepts by sampling training triples within each mini-batch. We introduce a variety of strategies for searching with the trained vector-space model, including approaches that incorporate domain-specific synonyms at search time with no model retraining. Across five datasets, our models that are trained only once on their corresponding ontologies are within 3 points of state-of-the-art models that are retrained for each new domain. Our models can also be trained for each domain, achieving new state-of-the-art on multiple datasets.",https://aclanthology.org/2021.bionlp-1.2,Association for Computational Linguistics,2021,June,Proceedings of the 20th Workshop on Biomedical Language Processing,"Xu, Dongfang  and
Bethard, Steven",Triplet-Trained Vector Space and Sieve-Based Search Improve Biomedical Concept Normalization,10.18653/v1/2021.bionlp-1.2,bionlp
2020.ccl-1.78,"['Data Management and Generation', 'Classification Applications', 'Low-resource Languages', 'Multilingual NLP', 'Model Architectures']",['Data Augmentation'],,"Neural network based models have achieved impressive results on the sentence classification task. However, most of previous work focuses on designing more sophisticated network or effective learning paradigms on monolingual data, which often suffers from insufficient discriminative knowledge for classification. In this paper, we investigate to improve sentence classification by multilingual data augmentation and consensus learning. Comparing to previous methods, our model can make use of multilingual data generated by machine translation and mine their language-share and language-specific knowledge for better representation and classification. We evaluate our model using English i.e., source language and Chinese i.e., target language data on several sentence classification tasks. Very positive classification performance can be achieved by our proposed model.",https://aclanthology.org/2020.ccl-1.78,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Wang, Yanfei  and
Chen, Yangdong  and
Zhang, Yuejie",Improving Sentence Classification by Multilingual Data Augmentation and Consensus Learning,,ccl
2020.cogalex-1.3,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Cross-linguistic studies of concepts provide valuable insights for the investigation of the mental lexicon. Recent developments of cross-linguistic databases facilitate an exploration of a diverse set of languages on the basis of comparative concepts. These databases make use of a wellestablished reference catalog, the Concepticon, which is built from concept lists published in linguistics. A recently released feature of the Concepticon includes data on norms, ratings, and relations for words and concepts. The present study used data on word frequencies to test two hypotheses. First, I examined the assumption that related languages i.e., English and German share concepts with more similar frequencies than non-related languages i.e., English and Chinese. Second, the variation of frequencies across both language pairs was explored to answer the question of whether the related languages share fewer concepts with a large difference between the frequency than the non-related languages. The findings indicate that related languages experience less variation in their frequencies. If there is variation, it seems to be due to cultural and structural differences. The implications of this study are far-reaching in that it exemplifies the use of cross-linguistic data for the study of the mental lexicon.",https://aclanthology.org/2020.cogalex-1.3,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Tjuka, Annika","General patterns and language variation: Word frequencies across English, German, and Chinese",,cogalex
2016.gwc-1.44,"['Knowledge Representation and Reasoning', 'Low-resource Languages']","['Semantic Web', 'Ontologies']",,This paper presents our first attempt at verifying integrity constraints of our openWordnet-PT against the ontology for Wordnets encoding. Our wordnet is distributed in Resource Description Format RDF and we want to guarantee not only the syntax correctness but also its semantics soundness.,https://aclanthology.org/2016.gwc-1.44,Global Wordnet Association,2016,27--30 January,Proceedings of the 8th Global WordNet Conference (GWC),"Rademaker, Alexandre  and
Chalub, Fabricio",Verifying Integrity Constraints of a RDF-based WordNet,,gwc
2020.alta-1.2,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"Cyberbullying is a prevalent and growing social problem due to the surge of social media technology usage. Minorities, women, and adolescents are among the common victims of cyberbullying. Despite the advancement of NLP technologies, the automated cyberbullying detection remains challenging. This paper focuses on advancing the technology using state-of-the-art NLP techniques. We use a Twitter dataset from SemEval 2019 -Task 5 HatEval on hate speech against women and immigrants. Our best performing ensemble model based on DistilBERT has achieved 0.73 and 0.74 of F1 score in the task of classifying hate speech Task A and aggressiveness and target Task B respectively. We adapt the ensemble model developed for Task A to classify offensive language in external datasets and achieved ∼0.7 of F1 score using three benchmark datasets, enabling promising results for cross-domain adaptability. We conduct a qualitative analysis of misclassified tweets to provide insightful recommendations for future cyberbullying research.",https://aclanthology.org/2020.alta-1.2,Australasian Language Technology Association,2020,December,Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,"Atapattu, Thushari  and
Herath, Mahen  and
Zhang, Georgia  and
Falkner, Katrina",Automated Detection of Cyberbullying Against Women and Immigrants and Cross-domain Adaptability,,alta
2020.onion-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['Data Preparation', 'Medical and Clinical NLP', 'Sentiment Analysis (SA)']",,"Humans frequently are able to read and interpret emotions of others by directly taking verbal and non-verbal signals in human-to-human communication into account or to infer or even experience emotions from mediated stories. For computers, however, emotion recognition is a complex problem: Thoughts and feelings are the roots of many behavioural responses and they are deeply entangled with neurophysiological changes within humans. As such, emotions are very subjective, often are expressed in a subtle manner, and are highly depending on context. For example, machine learning approaches for text-based sentiment analysis often rely on incorporating sentiment lexicons or language models to capture the contextual meaning. This paper explores if and how we further can enhance sentiment analysis using biofeedback of humans which are experiencing emotions while reading texts. Specifically, we record the heart rate and brain waves of readers that are presented with short texts which have been annotated with the emotions they induce. We use these physiological signals to improve the performance of a lexicon-based sentiment classifier. We find that the combination of several biosignals can improve the ability of a text-based classifier to detect the presence of a sentiment in a text on a per-sentence level.",https://aclanthology.org/2020.onion-1.5,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Schl{\""o}r, Daniel  and
Zehe, Albin  and
Kobs, Konstantin  and
Veseli, Blerta  and
Westermeier, Franziska  and
Br{\""u}bach, Larissa  and
Roth, Daniel  and
Latoschik, Marc Erich  and
Hotho, Andreas",Improving Sentiment Analysis with Biofeedback Data,,onion
2020.nlposs-1.18,"['Data Management and Generation', 'Learning Paradigms', 'Adversarial Attacks and Robustness']","['Adversarial Learning', 'Data Augmentation']",,"TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused across attacks. This framework allows both researchers and developers to test and study the weaknesses of their NLP models. To build such an open-source NLP toolkit requires solving some common problems: How do we enable users to supply models from different deep learning frameworks? How can we build tools to support as many different datasets as possible? We share our insights into developing a well-written, well-documented NLP Python framework in hope that they can aid future development of similar packages.",https://aclanthology.org/2020.nlposs-1.18,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Morris, John  and
Yoo, Jin Yong  and
Qi, Yanjun",TextAttack: Lessons learned in designing Python frameworks for NLP,10.18653/v1/2020.nlposs-1.18,nlposs
2020.findings-emnlp.295,"['Data Management and Generation', 'Error Detection and Correction', 'Audio Generation and Processing']","['Data Preparation', 'Automatic Speech Recognition (ASR)']",,"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates WERs achieved by modern Automatic Speech Recognition ASR systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.",https://aclanthology.org/2020.findings-emnlp.295,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Szyma{\'n}ski, Piotr  and
{\.Z}elasko, Piotr  and
Morzy, Mikolaj  and
Szymczak, Adrian  and
{\.Z}y{\l}a-Hoppe, Marzena  and
Banaszczak, Joanna  and
Augustyniak, Lukasz  and
Mizgajski, Jan  and
Carmiel, Yishay",WER we are and WER we think we are,10.18653/v1/2020.findings-emnlp.295,findings
2020.acl-main.385,"['Model Architectures', 'Embeddings']",['Transformer Models'],,"In the Transformer model, ""self-attention"" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",https://aclanthology.org/2020.acl-main.385,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Abnar, Samira  and
Zuidema, Willem",Quantifying Attention Flow in Transformers,10.18653/v1/2020.acl-main.385,acl
2020.ijclclp-1.1,"['Data Management and Generation', 'Error Detection and Correction', 'Low-resource Languages', 'Learning Paradigms', 'Model Architectures', 'Machine Translation (MT)']","['Recurrent Neural Networks (RNNs)', 'Supervised Learning', 'Data Augmentation', 'Neural MT (NMT)', 'Data Preparation']",,"We present a method for Chinese spelling check that automatically learns to correct a sentence with potential spelling errors. In our approach, a character-based neural machine translation NMT model is trained to translate the potentially misspelled sentence into correct one, using right-and-wrong sentence pairs from newspaper edit logs and artificially generated data. The method involves extracting sentences contain edit of spelling correction from edit logs, using commonly confused right-and-wrong word pairs to generate artificial right-and-wrong sentence pairs in order to expand our training data , and training the NMT model. The evaluation on the United Daily News UDN Edit Logs and SIGHAN-7 Shared Task shows that adding artificial error data can significantly improve the performance of Chinese spelling check system.",https://aclanthology.org/2020.ijclclp-1.1,Association for Computational Linguistics and Chinese Language Processing,2020,June,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 25, Number 1, June 2020","Chen, Jhih-Jie  and
Tu, Hai-Lun  and
Yang, Ching-Yu  and
Li, Chiao-Wen  and
Chang, Jason S.",Chinese Spelling Check based on Neural Machine Translation,,ijclclp
2019.icon-1.20,"['Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Unsupervised Learning', 'Data Preparation', 'Emotion Detection']",,"Existing supervised solutions for emotion classification demand large amount of emotion annotated data. Such resources may not be available for many languages. However, it is common to have sentiment annotated data available in these languages. The sentiment information +1 or -1 is useful to segregate between positive emotions or negative emotions. In this paper, we propose an unsupervised approach for emotion recognition by taking advantage of the sentiment information. Given a sentence and its sentiment information, recognize the best possible emotion for it. For every sentence, the semantic relatedness between the words from sentence and a set of emotionspecific words is calculated using cosine similarity. An emotion vector representing the emotion score for each emotion category of Ekman's model, is created. It is further improved with the dependency relations and the best possible emotion is predicted. The results show the significant improvement in f-score values for text with sentiment information as input over our baseline as text without sentiment information. We report the weighted fscore on three different datasets with the Ekman's emotion model. This supports that by leveraging the sentiment value, better emotion annotated data can be created.",https://aclanthology.org/2019.icon-1.20,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Kulkarni, Manasi  and
Bhattacharyya, Pushpak",Converting Sentiment Annotated Data to Emotion Annotated Data,,icon
2020.lincr-1.4,"['Evaluation Techniques', 'Low-resource Languages', 'Text Generation']",,,"Linguistics predictability is the degree of confidence in which language unit word, part of speech, etc. will be the next in the sequence. Experiments have shown that the correct prediction simplifies the perception of a language unit and its integration into the context. As a result of an incorrect prediction, language processing slows down. Currently, to get a measure of the language unit predictability, a neurolinguistic experiment known as a cloze task has to be conducted on a large number of participants. Cloze tasks are resource-consuming and are criticized by some researchers as an insufficiently valid measure of predictability. In this paper, we compare different language models that attempt to simulate human respondents' performance on the cloze task. Using a language model to create cloze task simulations would require significantly less time and conduct studies related to linguistic predictability.",https://aclanthology.org/2020.lincr-1.4,European Language Resources Association,2020,May,Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources,"Nikiforova, Anastasia  and
Pletenev, Sergey  and
Sinitsyna, Daria  and
Sorokin, Semen  and
Lopukhina, Anastasia  and
Howell, Nick",Language Models for Cloze Task Answer Generation in Russian,,lincr
2020.argmining-1.13,"['Argument Mining', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Computational models of argument quality AQ have focused primarily on assessing the overall quality or just one specific characteristic of an argument, such as its convincingness or its clarity. However, previous work has claimed that assessment based on theoretical dimensions of argumentation could benefit writers, but developing such models has been limited by the lack of annotated data. In this work, we describe GAQCorpus, the first large, domain-diverse annotated corpus of theory-based AQ. We discuss how we designed the annotation task to reliably collect a large number of judgments with crowdsourcing, formulating theory-based guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment.",https://aclanthology.org/2020.argmining-1.13,Association for Computational Linguistics,2020,December,Proceedings of the 7th Workshop on Argument Mining,"Ng, Lily  and
Lauscher, Anne  and
Tetreault, Joel  and
Napoles, Courtney",Creating a Domain-diverse Corpus for Theory-based Argument Quality Assessment,,argmining
2016.amta-researchers.6,"['Model Architectures', 'Low-resource Languages', 'Machine Translation (MT)']",,,"The objective of interactive translation prediction ITP, a paradigm of computer-aided translation, is to assist professional translators by offering context-based computer-generated suggestions as they type. While most state-of-the-art ITP systems are tightly coupled to a machine translation MT system often created ad-hoc for this purpose, our proposal follows a resourceagnostic approach, one that does not need access to the inner workings of the bilingual resources MT systems or any other bilingual resources used to generate the suggestions, thus allowing to include new resources almost seamlessly. As we do not expect the user to tolerate more than a few proposals each time, the set of potential suggestions need to be filtered and ranked; the resource-agnostic approach has been evaluated before using a set of intuitive length-based and position-based heuristics designed to determine which suggestions to show, achieving promising results. In this paper, we propose a more principled suggestion ranking approach using a regressor a multilayer perceptron that achieves significantly better results.",https://aclanthology.org/2016.amta-researchers.6,The Association for Machine Translation in the Americas,2016,October 28 - November 1,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,"Torregrosa, Daniel  and
P{\'e}rez-Ortiz, Juan Antonio  and
Forcada, Mikel",Ranking suggestions for black-box interactive translation prediction systems with multilayer perceptrons,,amta
2020.readi-1.12,"['Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Text Generation']","['Text Simplification', 'Data Analysis']",,"In text simplification and readability research, several features have been proposed to estimate or simplify a complex text, e.g., readability scores, sentence length, or proportion of POS tags. These features are however mainly developed for English. In this paper, we investigate their relevance for Czech, German, English, Spanish, and Italian text simplification corpora. Our multi-lingual and multi-domain corpus analysis shows that the relevance of different features for text simplification is different per corpora, language, and domain. For example, the relevance of the lexical complexity is different across all languages, the BLEU score across all domains, and 14 features within the web domain corpora. Overall, the negative statistical tests regarding the other features across and within domains and languages lead to the assumption that text simplification models may be transferable between different domains or different languages.",https://aclanthology.org/2020.readi-1.12,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Stodden, Regina  and
Kallmeyer, Laura",A multi-lingual and cross-domain analysis of features for text simplification,,readi
2020.lrec-1.606,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Stance Detection']",['NLP for Social Media'],"Entity framing is the selection of aspects of an entity to promote a particular viewpoint towards that entity. We investigate entity framing of political figures through the use of names and titles in German online discourse, enhancing current research in entity framing through titling and naming that concentrates on English only. We collect tweets that mention prominent German politicians and annotate them for stance. We find that the formality of naming in these tweets correlates positively with their stance. This confirms sociolinguistic observations that naming and titling can have a status-indicating function and suggests that this function is dominant in German tweets mentioning political figures. We also find that this status-indicating function is much weaker in tweets from users that are politically left-leaning than in tweets by right-leaning users. This is in line with observations from moral psychology that left-leaning and right-leaning users assign different importance to maintaining social hierarchies.",https://aclanthology.org/2020.lrec-1.606,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"van den Berg, Esther  and
Korfhage, Katharina  and
Ruppenhofer, Josef  and
Wiegand, Michael  and
Markert, Katja",Doctor Who? Framing Through Names and Titles in German,,lrec
2020.ccl-1.106,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Transformer Models']",,"Clickbait is a form of web content designed to attract attention and entice users to click on specific hyperlinks. The detection of clickbaits is an important task for online platforms to improve the quality of web content and the satisfaction of users. Clickbait detection is typically formed as a binary classification task based on the title and body of a webpage, and existing methods are mainly based on the content of title and the relevance between title and body. However, these methods ignore the stylistic patterns of titles, which can provide important clues on identifying clickbaits. In addition, they do not consider the interactions between the contexts within title and body, which are very important for measuring their relevance for clickbait detection. In this paper, we propose a clickbait detection approach with style-aware title modeling and coattention. Specifically, we use Transformers to learn content representations of title and body, and respectively compute two content-based clickbait scores for title and body based on their representations. In addition, we propose to use a character-level Transformer to learn a style-aware title representation by capturing the stylistic patterns of title, and we compute a title stylistic score based on this representation. Besides, we propose to use a co-attention network to model the relatedness between the contexts within title and body, and further enhance their representations by encoding the interaction information. We compute a title-body matching score based on the representations of title and body enhanced by their interactions. The final clickbait score is predicted by a weighted summation of the aforementioned four kinds of scores. Extensive experiments on two benchmark datasets show that our approach can effectively improve the performance of clickbait detection and consistently outperform many baseline methods.",https://aclanthology.org/2020.ccl-1.106,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Wu, Chuhan  and
Wu, Fangzhao  and
Qi, Tao  and
Huang, Yongfeng",Clickbait Detection with Style-aware Title Modeling and Co-attention,,ccl
2020.semeval-1.61,"['Commonsense Reasoning', 'Model Architectures', 'Classification Applications']",['Transformer Models'],,"In this paper, we describe our system for Task 4 of SemEval 2020, which involves differentiating between natural language statements that confirm to common sense and those that do not. The organizers propose three subtasks -first, selecting between two sentences, the one which is against common sense. Second, identifying the most crucial reason why a statement does not make sense. Third, generating novel reasons for explaining the against common sense statement. Out of the three subtasks, this paper reports the system description of subtask A and subtask B. This paper proposes a model based on transformer neural network architecture for addressing the subtasks. The novelty in work lies in the architecture design, which handles the logical implication of contradicting statements and simultaneous information extraction from both sentences. We use a parallel instance of transformers, which is responsible for a boost in the performance. We achieved an accuracy of 94.8% in subtask A and 89% in subtask B on the test set.",https://aclanthology.org/2020.semeval-1.61,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Dash, Soumya Ranjan  and
Routray, Sandeep  and
Varshney, Prateek  and
Modi, Ashutosh",CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE,10.18653/v1/2020.semeval-1.61,semeval
2020.osact-1.17,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection', 'Data Augmentation']",['NLP for Social Media'],"Social media are pervasive in our life, making it necessary to ensure safe online experiences by detecting and removing offensive and hate speech. In this work, we report our submission to the Offensive Language and hate-speech Detection shared task organized with the 4 th Workshop on Open-Source Arabic Corpora and Processing Tools Arabic OSACT4. We focus on developing purely deep learning systems, without a need for feature engineering. For that purpose, we develop an effective method for automatic data augmentation and show the utility of training both offensive and hate speech models off i.e., by fine-tuning previously trained affective models i.e., sentiment and emotion. Our best models are significantly better than a vanilla BERT model, with 89.60% acc 82.31% macro F1 for hate speech and 95.20% acc 70.51% macro F1 on official TEST data.",https://aclanthology.org/2020.osact-1.17,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Elmadany, AbdelRahim  and
Zhang, Chiyu  and
Abdul-Mageed, Muhammad  and
Hashemi, Azadeh",Leveraging Affective Bidirectional Transformers for Offensive Language Detection,,osact
2016.amta-researchers.2,['Machine Translation (MT)'],,,"We assessed how different machine translation MT systems affect the post-editing PE process and product of professional English-Spanish translators. Our model found that for each 1-point increase in BLEU, there is a PE time decrease of 0.16 seconds per word, about 3-4%. The MT system with the lowest BLEU score produced the output that was post-edited to the lowest quality and with the highest PE effort, measured both in HTER and actual PE operations.",https://aclanthology.org/2016.amta-researchers.2,The Association for Machine Translation in the Americas,2016,October 28 - November 1,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,"Sanchez-Torron, Marina  and
Koehn, Philipp",Machine Translation Quality and Post-Editor Productivity,,amta
2020.findings-emnlp.326,"['Argument Mining', 'Parsing', 'Information Extraction', 'Model Architectures']","['Syntactic Parsing', 'Semantic Parsing', 'Event Extraction']",,"The goal of Event Argument Extraction EAE is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks GTNs to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.",https://aclanthology.org/2020.findings-emnlp.326,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Pouran Ben Veyseh, Amir  and
Nguyen, Tuan Ngo  and
Nguyen, Thien Huu",Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction,10.18653/v1/2020.findings-emnlp.326,findings
2020.alvr-1.4,"['Evaluation Techniques', 'Image and Video Processing', 'Dialogue Systems']",,,"Task success is the standard metric used to evaluate referential visual dialogue systems. In this paper we propose two new metrics that evaluate how each question contributes to the goal. First, we measure how effective each question is by evaluating whether the question discards objects that are not the referent. Second, we define referring questions as those that univocally identify one object in the image. We report the new metrics for human dialogues and for state of the art publicly available models on GuessWhat?!. Regarding our first metric, we find that successful dialogues do not have a higher percentage of effective questions for most models. With respect to the second metric, humans make questions at the end of the dialogue that are referring, confirming their guess before guessing. Human dialogues that use this strategy have a higher task success but models do not seem to learn it.",https://aclanthology.org/2020.alvr-1.4,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Advances in Language and Vision Research,"Mazuecos, Mauricio  and
Testoni, Alberto  and
Bernardi, Raffaella  and
Benotti, Luciana",On the role of effective and referring questions in GuessWhat?!,10.18653/v1/2020.alvr-1.4,alvr
2020.pam-1.15,"['Data Management and Generation', 'Model Architectures']",['Data Preparation'],,"Natural Language Inference models have reached almost human-level performance but their generalisation capabilities have not been yet fully characterized. In particular, sensitivity to small changes in the data is a current area of investigation. In this paper, we focus on the effect of punctuation on such models. Our findings can be broadly summarized as follows: 1 irrelevant changes in punctuation are correctly ignored by the recent transformer models BERT while older RNN-based models were sensitive to them. 2 All models, both transformers and RNN-based models, are incapable of taking into account small relevant changes in the punctuation.",https://aclanthology.org/2020.pam-1.15,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"Ek, Adam  and
Bernardy, Jean-Philippe  and
Chatzikyriakidis, Stergios",How does Punctuation Affect Neural Models in Natural Language Inference,,pam
2020.cmlc-1.3,"['Parsing', 'Information Extraction', 'Data Management and Generation', 'Embeddings', 'Low-resource Languages', 'Text Preprocessing']","['Part-of-Speech (POS) Tagging', 'Word Embeddings', 'Named Entity Recognition (NER)', 'Syntactic Parsing', 'Data Preparation']",['Dependency Parsing'],"This paper investigates the impact of different types and size of training corpora on language models. By asking the fundamental question of quality versus quantity, we compare four French corpora by pre-training four different ELMOs and evaluating them on dependency parsing, POS-tagging and Named Entities Recognition downstream tasks. We present and asses the relevance of a new balanced French corpus, CaBeRnet, that features a representative range of language usage, including a balanced variety of genres oral transcriptions, newspapers, popular magazines, technical reports, fiction, academic texts, in oral and written styles. We hypothesize that a linguistically representative corpus will allow the language models to be more efficient, and therefore yield better evaluation scores on different evaluation sets and tasks.",https://aclanthology.org/2020.cmlc-1.3,European Language Ressources Association,2020,May,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,"Popa-Fabre, Murielle  and
Ortiz Su{\'a}rez, Pedro Javier  and
Sagot, Beno{\^\i}t  and
de la Clergerie, {\'E}ric",French Contextualized Word-Embeddings with a sip of CaBeRnet: a New French Balanced Reference Corpus,,cmlc
2019.icon-1.24,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms', 'Model Architectures']","['NLP for News and Media', 'Recurrent Neural Networks (RNNs)', 'Supervised Learning', 'Sarcasm Detection', 'Data Preparation']","['Long Short-Term Memory (LSTM) Models', 'NLP for Social Media']","Self-deprecating sarcasm is a special category of sarcasm, which is nowadays popular and useful for many real-life applications, such as brand endorsement, product campaign, digital marketing, and advertisement. The selfdeprecating style of campaign and marketing strategy is mainly adopted to excel brand endorsement and product sales value. In this paper, we propose an LSTM-based deep learning approach for detecting self-deprecating sarcasm in textual data. To the best of our knowledge, there is no prior work related to self-deprecating sarcasm detection using deep learning techniques. Starting with a filtering step to identify self-referential tweets, the proposed approach adopts a deep learning model using LSTM for detecting self-deprecating sarcasm. The proposed approach is evaluated over three Twitter datasets and performs significantly better in terms of precision, recall, and f-score.",https://aclanthology.org/2019.icon-1.24,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Kamal, Ashraf  and
Abulaish, Muhammad",An LSTM-Based Deep Learning Approach for Detecting Self-Deprecating Sarcasm in Textual Data,,icon
2021.clpsych-1.1,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Ethics']","['NLP for News and Media', 'Data Preparation', 'Medical and Clinical NLP']","['NLP for Social Media', 'Annotation Processes', 'NLP for Mental Health']","Recently, research on mental health conditions using public online data, including Reddit, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine-than masculinegendered mainly young or middle-aged USbased adults who often report additional mental health diagnoses, which is compared with general Reddit statistics and epidemiological studies. Additionally, this paper carefully evaluates all methods and discusses ethical issues.",https://aclanthology.org/2021.clpsych-1.1,Association for Computational Linguistics,2021,June,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,"Jagfeld, Glorianna  and
Lobban, Fiona  and
Rayson, Paul  and
Jones, Steven",Understanding who uses Reddit: Profiling individuals with a self-reported bipolar disorder diagnosis,10.18653/v1/2021.clpsych-1.1,clpsych
2021.blackboxnlp-1.2,"['Parsing', 'Text Preprocessing']","['Part-of-Speech (POS) Tagging', 'Syntactic Parsing']",['Dependency Parsing'],"Previous work on probing word representations for linguistic knowledge has focused on interpolation tasks. In this paper, we instead analyse probes in an extrapolation setting, where the inputs at test time are deliberately chosen to be 'harder' than the training examples. We argue that such an analysis can shed further light on the open question whether probes actually decode linguistic knowledge, or merely learn the diagnostic task from shallow features. To quantify the hardness of an example, we consider scoring functions based on linguistic, statistical, and learning-related criteria, all of which are applicable to a broad range of NLP tasks. We discuss the relative merits of these criteria in the context of two syntactic probing tasks, part-of-speech tagging and syntactic dependency labelling. From our theoretical and experimental analysis, we conclude that distance-based and hard statistical criteria show the clearest differences between interpolation and extrapolation settings, while at the same time being transparent, intuitive, and easy to control.",https://aclanthology.org/2021.blackboxnlp-1.2,Association for Computational Linguistics,2021,November,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Kunz, Jenny  and
Kuhlmann, Marco",Test Harder than You Train: Probing with Extrapolation Splits,10.18653/v1/2021.blackboxnlp-1.2,blackboxnlp
2020.nlposs-1.20,['Evaluation Techniques'],,,"For the last 5 years, we have developed and maintained RSMTool -an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its crossdisciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper. 1 https://www.measurementinc.com/ products-services/automated-essay-scoring 2 https://www.ets.org/accelerate/ai-portfolio/ speechrater be evaluated as thoroughly as possible to detect any harmful, systematic biases in their predictions. However, this may prove difficult for an NLP or machine learning researcher since they may be unfamiliar with the required psychometric and statistical checks. RSMTool incorporates a large, diverse set of psychometric and statistical analyses aimed at detecting possible bias in system performance and makes them available in an easy-to-use package. RSMTool is open-source and non-proprietary so that the automated scoring community can not only audit the source code of the already available analyses to ensure their compliance with fairness standards but also contribute new analyses.",https://aclanthology.org/2020.nlposs-1.20,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Madnani, Nitin  and
Loukina, Anastassia",User-centered \& Robust NLP OSS: Lessons Learned from Developing \& Maintaining RSMTool,10.18653/v1/2020.nlposs-1.20,nlposs
2021.bppf-1.3,"['Evaluation Techniques', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Evaluation is of paramount importance in datadriven research fields such as Natural Language Processing NLP and Computer Vision CV. But current evaluation practice in NLP, except for end-to-end tasks such as machine translation, spoken dialogue systems, or NLG, largely hinges on the existence of a single ""ground truth"" against which we can meaningfully compare the prediction of a model. However, this assumption is flawed for two reasons. 1 In many cases, more than one answer is correct. 2 Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Current methods of adjudication, agreement, and evaluation ought to be reconsidered at the light of this evidence. Some researchers now propose to address this issue by minimizing disagreement, creating cleaner datasets. We argue that such a simplification is likely to result in oversimplified models just as much as it would do for end-to-end tasks such as machine translation. Instead, we suggest that we need to improve today's evaluation practice to better capture such disagreement. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation.",https://aclanthology.org/2021.bppf-1.3,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future","Basile, Valerio  and
Fell, Michael  and
Fornaciari, Tommaso  and
Hovy, Dirk  and
Paun, Silviu  and
Plank, Barbara  and
Poesio, Massimo  and
Uma, Alexandra",We Need to Consider Disagreement in Evaluation,10.18653/v1/2021.bppf-1.3,bppf
2020.ngt-1.6,"['Learning Paradigms', 'Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Supervised Learning', 'Data Preparation', 'Paraphrase and Rephrase Generation']",,"The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first corpus of such type in Russian computational linguistics. Existing manually annotated paraphrase datasets for Russian are limited to small-sized ParaPhraser corpus and ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and plagiarism detection, sentence similarity and relatedness estimation, etc. Due to size restrictions, these datasets can hardly be applied in end-to-end text generation solutions. Meanwhile, paraphrase generation requires a large amount of training data. In our study we propose a solution to the problem: we collect, rank and evaluate a new publicly available headline paraphrase corpus ParaPhraser Plus, and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.",https://aclanthology.org/2020.ngt-1.6,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Gudkov, Vadim  and
Mitrofanova, Olga  and
Filippskikh, Elizaveta",Automatically Ranked Russian Paraphrase Corpus for Text Generation,10.18653/v1/2020.ngt-1.6,ngt
2016.lilt-14.4,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"In this paper we present current work on the design and validation of a linguistically-motivated annotation model of modality in English and Spanish in the context of the MULTINOT project. 1 Our annotation model captures four basic modal meanings and their subtypes, on the one hand, and provides a fine-grained characterisation of the syntactic realisations of those meanings in English and Spanish, on the other. We validate the modal tagset proposed through an agreement study performed on a bilingual sample of four hundred sentences extracted from original texts of the MULTINOT corpus, and discuss the difficult cases encountered in the annotation experiment. We also describe current steps in the implementation of the proposed scheme for the large-scale annotation of the bilingual corpus using both automatic and manual procedures. 1 The MULTINOT project is financed by the Spanish Ministry of Economy and Competitiveness, under grant number FF2012-32201. We gratefully acknowledge the support provided by the Spanish authorities. We also thank the comments and suggestions provided by the anonymous reviewers which have helped to improve the current manuscript.",https://aclanthology.org/2016.lilt-14.4,CSLI Publications,2016,sept,"Linguistic Issues in Language Technology, Volume 14, 2016 - Modality: Logic, Semantics, Annotation, and Machine Learning","Lavid, Julia  and
Carretrero, Marta  and
Zamorano-Mansilla, Juan Rafael",A linguistically-motivated annotation model of modality in English and Spanish: Insights from MULTINOT,,lilt
2020.cogalex-1.6,"['Multilingual NLP', 'Cross-lingual Application', 'Low-resource Languages', 'Classification Applications']",,,"The HSemID system, submitted to the CogALex VI Shared Task is a hybrid system relying mainly on metric clusters measured in large web corpora, complemented by a vector space model using cosine similarity to detect semantic associations. Although the system reached rather weak results for the subcategories of synonyms, antonyms and hypernyms, with some differences from one language to another, it is able to measure general semantic associations as being random or not-random with an F1 score close to 0.80. The results strongly suggest that idiomatic constructions play a fundamental role in semantic associations. Further experiments are necessary in order to fine-tune the model to the subcategories of synonyms, antonyms, hypernyms and to explain surprising differences across languages.",https://aclanthology.org/2020.cogalex-1.6,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Colson, Jean-Pierre",Extracting meaning by idiomaticity: Description of the HSemID system at CogALex VI 2020,,cogalex
2020.crac-1.2,"['Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Low-resource Languages']","['Sentiment Analysis (SA)', 'Data Preparation', 'Coreference Resolution']",['Aspect-Based SA (ABSA)'],"While it has been claimed that anaphora or coreference resolution plays an important role in opinion mining, it is not clear to what extent coreference resolution actually boosts performance, if at all. In this paper, we investigate the potential added value of coreference resolution for the aspect-based sentiment analysis of restaurant reviews in two languages, English and Dutch. We focus on the task of aspect category classification and investigate whether including coreference information prior to classification to resolve implicit aspect mentions is beneficial. Because coreference resolution is not a solved task in NLP, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a classifier on a combination of lexical and semantic features, we show that resolving the coreferential relations prior to classification is beneficial in a joint optimization setup. However, this is only the case when relying on gold-standard relations and the result is more outspoken for English than for Dutch. When validating the optimal models, however, we found that only the Dutch pipeline is able to achieve a satisfying performance on a held-out test set and does so regardless of whether coreference information was included.",https://aclanthology.org/2020.crac-1.2,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference","De Clercq, Orphee  and
Hoste, Veronique",It's absolutely divine! Can fine-grained sentiment analysis benefit from coreference resolution?,,crac
2017.mtsummit-papers.3,"['Model Architectures', 'Low-resource Languages', 'Machine Translation (MT)']",,,"Hybrid machine translation (HMT) takes advantage of different types of machine translation (MT) systems to improve translation performance. Neural machine translation (NMT) can produce more fluent translations while phrase-based statistical machine translation (PB-SMT) can produce adequate results primarily due to the contribution of the translation model. In this paper, we propose a cascaded hybrid framework to combine NMT and PB-SMT to improve translation quality. Specifically, we first use the trained NMT system to pre-translate the training data, and then employ the pre-translated training data to build an SMT system and tune parameters using the pre-translated development set. Finally, the SMT system is utilised as a post-processing step to re-decode the pre-translated test set and produce the final result. Experiments conducted on Japanese→English and Chinese→English show that the proposed cascaded hybrid framework can significantly improve performance by 2.38 BLEU points and 4.22 BLEU points, respectively, compared to the baseline NMT system.",https://aclanthology.org/2017.mtsummit-papers.3,,2017,September 18 {--} September 22,Proceedings of Machine Translation Summit XVI: Research Track,"Du, Jinhua  and
Way, Andy",Neural Pre-Translation for Hybrid Machine Translation,,mtsummit
2020.alvr-1.5,"['Question Answering (QA)', 'Image and Video Processing', 'Model Architectures', 'Learning Paradigms']",['Multimodal Learning'],,"We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA. Our model is solving the textual cloze task which is a reading comprehension on a recipe containing images and instructions. We exploit the power of attention networks, cross-modal representations, and a latent alignment space between instructions and candidate answers to solve the problem. We introduce constrained max-pooling which refines the max-pooling operation on the alignment matrix to impose disjoint constraints among the outputs of the model. Our evaluation result indicates a 19% improvement over the baselines.",https://aclanthology.org/2020.alvr-1.5,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Advances in Language and Vision Research,"Rajaby Faghihi, Hossein  and
Mirzaee, Roshanak  and
Paliwal, Sudarshan  and
Kordjamshidi, Parisa",Latent Alignment of Procedural Concepts in Multimodal Recipes,10.18653/v1/2020.alvr-1.5,alvr
2020.sustainlp-1.21,"['Finite State Machines', 'Model Architectures', 'Classification Applications']",['Sentiment Analysis (SA)'],,"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata WFA. Each model has its advantages and disadvantages and it is possible that they could be combined. However, we believe that the first research goal should be to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs.",https://aclanthology.org/2020.sustainlp-1.21,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Quattoni, Ariadna  and
Carreras, Xavier",A comparison between CNNs and WFAs for Sequence Classification,10.18653/v1/2020.sustainlp-1.21,sustainlp
2020.globalex-1.18,"['Low-resource Languages', 'Machine Translation (MT)']",,,"This paper describes four different strategies proposed to the TIAD 2020 Shared Task for automatic translation inference across dictionaries. The proposed strategies are based on the analysis of Apertium RDF graph, taking advantage of characteristics such as translation using multiple paths, synonyms and similarities between lexical entries from different lexicons and cardinality of possible translations through the graph. The four strategies were trained and validated on the Apertium RDF EN ↔ ES dictionary, showing promising results. Finally, the strategies, applied together, obtained an F-measure of 0.43 in the task of inferring the dictionaries proposed in the shared task, ranking thus third with respect to the other new systems presented to the TIAD 2020 Shared Task. No system presented to the shared task exceeded the baseline proposed by the TIAD organizers.",https://aclanthology.org/2020.globalex-1.18,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Dranca, Lacramioara",Multi-Strategy system for translation inference across dictionaries,,globalex
2020.paclic-1.40,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Concerning the acquisition of relative clauses RCs, studies on head-initial languages consistently reported a preference for subjectgapped RCs, but the issue of subject-object asymmetry is still a controversial one in research on the acquisition of RCs in head-final languages. Using written corpus data, this study investigated the second language production of RCs in Mandarin Chinese Chinese by Japanese-speaking and Thai-speaking learners with various proficiency levels. We first extracted the RCs produced by Japanese and Thai learners from the HKS Dynamic Composition Corpus, and coded head type and gap type for further analyses. The learners from the intermediate-level groups produced a significant number of error-free RCs, which suggests that the intermediate learners have already mastered Chinese RCs. Both Japanese and Thai learners exhibited a strong preference for the subject RCs, which is consistent with predictions that follow from the Noun Phrase Accessibility Hierarchy NPAH and the results of studies on head-initial languages. Our data also provided partial support for the Subject-Object Hierarchy SOH. However, the size of the corpus was insufficient to exhaustively investigate the tested theories. More data are needed to examine the applicability of the NPAH and SOH hypotheses in L2 Chinese and in general.",https://aclanthology.org/2020.paclic-1.40,Association for Computational Linguistics,2020,October,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation","Yang, Yike",A corpus-based analysis of Chinese relative clauses produced by Japanese and Thai learners,,paclic
2020.globalex-1.2,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],,"This paper reports on an extended version of a synonym verb class lexicon, newly called SynSemClass formerly CzEngClass. This lexicon stores cross-lingual semantically similar verb senses in synonym classes extracted from a richly annotated parallel corpus, the Prague Czech-English Dependency Treebank. When building the lexicon, we make use of predicate-argument relations valency and link them to semantic roles; in addition, each entry is linked to several external lexicons of more or less ""semantic"" nature, namely FrameNet, WordNet, VerbNet, OntoNotes and PropBank, and Czech VALLEX. The aim is to provide a linguistic resource that can be used to compare semantic roles and their syntactic properties and features across languages within and across synonym groups classes, or 'synsets', as well as gold standard data for automatic NLP experiments with such synonyms, such as synonym discovery, feature mapping, etc. However, perhaps the most important goal is to eventually build an event type ontology that can be referenced and used as a human-readable and human-understandable ""database"" for all types of events, processes and states. While the current paper describes primarily the content of the lexicon, we are also presenting a preliminary design of a format compatible with Linked Data, on which we are hoping to get feedback during discussions at the workshop. Once the resource in whichever form is applied to corpus annotation, deep analysis will be possible using such combined resources as training data.",https://aclanthology.org/2020.globalex-1.2,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Uresova, Zdenka  and
Fucikova, Eva  and
Hajicova, Eva  and
Hajic, Jan",SynSemClass Linked Lexicon: Mapping Synonymy between Languages,,globalex
2020.coling-demos.14,"['Data Management and Generation', 'Dialogue Systems']","['Data Preparation', 'Chatbots']",['Annotation Processes'],"In this paper, we introduce Annobot: a platform for annotating and creating datasets through conversation with a chatbot. This natural form of interaction has allowed us to create a more accessible and flexible interface, especially for mobile devices. Our solution has a wide range of applications such as data labelling for binary, multi-class/label classification tasks, preparing data for regression problems, or creating sets for issues such as machine translation, question answering or text summarization. Additional features include pre-annotation, active sampling, online learning and real-time inter-annotator agreement. The system is integrated with the popular messaging platform: Facebook Messanger. Usability experiment showed the advantages of the proposed platform compared to other labelling tools. The source code of Annobot is available under the GNU LGPL license at https://github.com/rafalposwiata/annobot.",https://aclanthology.org/2020.coling-demos.14,International Committee on Computational Linguistics (ICCL),2020,December,Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations,"Po{\'s}wiata, Rafa{\l}  and
Pere{\l}kiewicz, Micha{\l}",Annobot: Platform for Annotating and Creating Datasets through Conversation with a Chatbot,10.18653/v1/2020.coling-demos.14,coling
2020.findings-emnlp.127,"['Model Architectures', 'Dialogue Systems', 'Information Retrieval', 'Knowledge Representation and Reasoning']","['Recurrent Neural Networks (RNNs)', 'Information Filtering', 'Chatbots']",['Long Short-Term Memory (LSTM) Models'],"The challenges of building knowledgegrounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method named Filtering before Iteratively REferring FIRE for this task. In this method, a context filter and a knowledge filter are first built, which derive knowledge-aware context representations and context-aware knowledge representations respectively by global and bidirectional attention. Besides, the entries irrelevant to the conversation are discarded by the knowledge filter. After that, iteratively referring is performed between context and response representations as well as between knowledge and response representations, in order to collect deep matching features for scoring response candidates. Experimental results show that FIRE outperforms previous methods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with original and revised personas respectively, and margins larger than 3.1% on the CMU DoG dataset in terms of top-1 accuracy. We also show that FIRE is more interpretable by visualizing the knowledge grounding process.",https://aclanthology.org/2020.findings-emnlp.127,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Gu, Jia-Chen  and
Ling, Zhenhua  and
Liu, Quan  and
Chen, Zhigang  and
Zhu, Xiaodan",Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots,10.18653/v1/2020.findings-emnlp.127,findings
2020.autosimtrans-1.2,"['Model Architectures', 'Low-resource Languages', 'Audio Generation and Processing', 'Learning Paradigms']",['Adversarial Learning'],,"End-to-end speech translation usually leverages audio-to-text parallel data to train an available speech translation model which has shown impressive results on various speech translation tasks. Due to the artificial cost of collecting audio-to-text parallel data, the speech translation is a natural low-resource translation scenario, which greatly hinders its improvement. In this paper, we proposed a new adversarial training method to leverage target monolingual data to relieve the lowresource shortcoming of speech translation. In our method, the existing speech translation model is considered as a Generator to gain a target language output, and another neural Discriminator is used to guide the distinction between outputs of speech translation model and true target monolingual sentences. Experimental results on the CCMT 2019-BSTC dataset speech translation task demonstrate that the proposed methods can significantly improve the performance of the end-to-end speech translation.",https://aclanthology.org/2020.autosimtrans-1.2,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Automatic Simultaneous Translation,"Li, Xuancai  and
Kehai, Chen  and
Zhao, Tiejun  and
Yang, Muyun",End-to-End Speech Translation with Adversarial Training,10.18653/v1/2020.autosimtrans-1.2,autosimtrans
2020.aacl-srw.23,['Text Generation'],,,"Several recent state-of-the-art transfer learning methods model classification tasks as text generation, where labels are represented as strings for the model to generate. We investigate the effect that the choice of strings used to represent labels has on how effectively the model learns the task. For four standard text classification tasks, we design a diverse set of possible string representations for labels, ranging from canonical label definitions to random strings. We experiment with T5 Raffel et al., 2019 on these tasks, varying the label representations as well as the amount of training data. We find that, in the low data setting, label representation impacts task performance on some tasks, with task-related labels being most effective, but fails to have an impact on others. In the full data setting, our results are largely negative: Different label representations do not affect overall task performance.",https://aclanthology.org/2020.aacl-srw.23,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,"Chen, Xinyi  and
Xu, Jingxian  and
Wang, Alex",Label Representations in Modeling Classification as Text Generation,,aacl
2020.stoc-1.2,"['Dialogue Systems', 'Classification Applications']","['Email Spam and Phishing Detection', 'Response Generation']",,We present a paradigm for extensible lexicon development based on Lexical Conceptual Structure to support social engineering detection and response generation. We leverage the central notions of ask elicitation of behaviors such as providing access to money and framing risk/reward implied by the ask. We demonstrate improvements in ask/framing detection through refinements to our lexical organization and show that response generation qualitatively improves as ask/framing detection performance improves. The paradigm presents a systematic and efficient approach to resource adaptation for improved task-specific performance.,https://aclanthology.org/2020.stoc-1.2,European Language Resources Association,2020,May,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,"Bhatia, Archna  and
Dalton, Adam  and
Mather, Brodie  and
Santhanam, Sashank  and
Shaikh, Samira  and
Zemel, Alan  and
Strzalkowski, Tomek  and
Dorr, Bonnie J.",Adaptation of a Lexical Organization for Social Engineering Detection and Response Generation,,stoc
2020.nli-1.1,"['Question Answering (QA)', 'Model Architectures', 'Learning Paradigms']","['Knowledge Base QA', 'Supervised Learning']",,"Knowledge-based question answering KB-QA has long focused on simple questions that can be answered from a single knowledge source, a manually curated or an automatically extracted KB. In this work, we look at answering complex questions which often require combining information from multiple sources. We present a novel KB-QA system, MULTIQUE, which can map a complex question to a complex query pattern using a sequence of simple queries each targeted at a specific KB. It finds simple queries using a neural-network based model capable of collective inference over textual relations in extracted KB and ontological relations in curated KB. Experiments show that our proposed system outperforms previous KB-QA systems on benchmark datasets, ComplexWebQuestions and WebQuestionsSP.",https://aclanthology.org/2020.nli-1.1,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Interfaces,"Bhutani, Nikita  and
Zheng, Xinyi  and
Qian, Kun  and
Li, Yunyao  and
Jagadish, H.",Answering Complex Questions by Combining Information from Curated and Extracted Knowledge Bases,10.18653/v1/2020.nli-1.1,nli
2019.nsurl-1.3,"['Model Architectures', 'Audio Generation and Processing', 'Low-resource Languages', 'Classification Applications']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Gender recognition in speech processing is one of the most challenging tasks. While many studies rely on extracting features and designing enhancement classifiers, classification accuracy is still not satisfactory. The remarkable improvement in performance achieved through the use of neural networks for automatic speech recognition has encouraged the use of deep neural networks in other voice techniques such as speech, emotion, language and gender recognition. An earlier study showed a significant improvement in the gender recognition of pictures and videos. In this paper, speech is used to create a gender recognition scheme based on neural networks. Attention-based BiLSTM architecture is proposed to discover the best approach for gender identification in Yorùbá. Acoustic features, including time, frequency, and cepstral features are extracted to train the model. The model obtained the state-of-the-art performance in speech-based gender recognition with 99% accuracy and F 1 score.",https://aclanthology.org/2019.nsurl-1.3,Association for Computational Linguistics,2019,11--12 September,Proceedings of The First International Workshop on NLP Solutions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019 - Short Papers,"Modupe, Ibukunola Abosede  and
Sefara, Tshephisho Joseph  and
Sunday, Ojo",Yor\`ub\'a Gender Recognition from Speech using Attention-based BiLSTM,,nsurl
2020.multilingualbio-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Medical and Clinical NLP'],,"Medical language exhibits large variations regarding users, institutions, and language registers. With large parts of clinical information only documented in free text, NLP plays an important role in unlocking potentially re-usable and interoperable meaning from medical records in a multitude of natural languages. This study highlights the role of interface vocabularies. It describes the architectural principles and the evolution of a German interface vocabulary, which is under development by combining machine translation with human annotation and rule-based term generation, yielding a resource with 7.7 million raw entries, each of which linked to the reference terminology SNOMED CT, an international standard with about 350 thousand concepts. The purpose is to offer a high coverage of German medical jargon, in order to optimise terminology grounding of clinical texts by NLP systems. The core resource is a manually maintained table of English-to-German word and chunk translations, supported by a set of language generation rules. We describe a workflow consisting in the enrichment and modification of this table by human and machine efforts, together with top-down and bottomup methods for terminology population. A term generator generates the final vocabulary by creating one-to-many German variants per SNOMED CT English description. Filtering against a large collection of domain terminologies and corpora drastically reduces the size of the vocabulary in favour of terms that can reasonably be expected to match clinical text passages within a text-mining pipeline. An evaluation was performed by a comparison between the current version of the German interface vocabulary and the English description table of the SNOMED CT International release. An exact term matching was performed with a small parallel corpus constituted by text snippets from different clinical documents. With overall low retrieval parameters with F-values around 30%, the performance of the German language scenario reaches 80 -90% of the English one. Interestingly, annotations are slightly better with machine-translated German -English texts, using the International SNOMED CT resource only.",https://aclanthology.org/2020.multilingualbio-1.3,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020),"Schulz, Stefan  and
Hammer, Larissa  and
Hashemian-Nik, David  and
Kreuzthaler, Markus",Localising the Clinical Terminology SNOMED CT by Semi-automated Creation of a German Interface Vocabulary,,multilingualbio
2018.gwc-1.38,"['Knowledge Representation and Reasoning', 'Low-resource Languages', 'Machine Translation (MT)']",['Taxonomy Construction'],,"One of the fundamental building blocks of a wordnet is synonym sets or synsets, which group together similar word meanings or synonyms. These synsets can consist either one or more synonyms. This paper describes an automatic method for composing synsets with multiple synonyms by using Google Translate and Semantic Mirrors' method. Also, we will give an overview of the results and discuss the advantages of the proposed method from wordnet's point of view.",https://aclanthology.org/2018.gwc-1.38,Global Wordnet Association,2018,January,Proceedings of the 9th Global Wordnet Conference,"Lohk, Ahti  and
Tombak, Mati  and
Vare, Kadri",An Experiment: Using Google Translate and Semantic Mirrors to Create Synsets with Many Lexical Units,,gwc
2020.aacl-main.82,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Relation Extraction', 'Entity Linking', 'Named Entity Recognition (NER)', 'Data Preparation']","['Long Short-Term Memory (LSTM) Models', 'Annotation Processes']","We propose a newly annotated dataset for information extraction on recipes. Unlike previous approaches to machine comprehension of procedural texts, we avoid a priori pre-defining domain-specific predicates to recognize e.g., the primitive instructions in MILK and focus on basic understanding of the expressed semantics rather than directly reduce them to a simplified state representation e.g., ProPara. We thus frame the semantic comprehension of procedural text such as recipes, as fairly generic NLP subtasks, covering i entity recognition ingredients, tools and actions, ii relation extraction what ingredients and tools are involved in the actions, and iii zero anaphora resolution link actions to implicit arguments, e.g., results from previous recipe steps. Further, our Recipe Instruction Semantic Corpus RISeC dataset includes textual descriptions for the zero anaphora, to facilitate language generation thereof. Besides the dataset itself, we contribute a pipeline neural architecture that addresses entity and relation extraction as well as identification of zero anaphora. These basic building blocks can facilitate more advanced downstream applications e.g., question answering, conversational agents.",https://aclanthology.org/2020.aacl-main.82,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Jiang, Yiwei  and
Zaporojets, Klim  and
Deleu, Johannes  and
Demeester, Thomas  and
Develder, Chris",Recipe Instruction Semantics Corpus RISeC: Resolving Semantic Structure and Zero Anaphora in Recipes,,aacl
2020.osact-1.9,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"In this paper, we describe our efforts at OSACT Shared Task on Offensive Language Detection. The shared task consists of two subtasks: offensive language detection Subtask A and hate speech detection Subtask B. For offensive language detection, a system combination of Support Vector Machines SVMs and Deep Neural Networks DNNs achieved the best results on development set, which ranked 1st in the official results for Subtask A with F1-score of 90.51% on the test set. For hate speech detection, DNNs were less effective and a system combination of multiple SVMs with different parameters achieved the best results on development set, which ranked 4th in official results for Subtask B with F1-macro score of 80.63% on the test set.",https://aclanthology.org/2020.osact-1.9,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Hassan, Sabit  and
Samih, Younes  and
Mubarak, Hamdy  and
Abdelali, Ahmed  and
Rashed, Ammar  and
Chowdhury, Shammur Absar",ALT Submission for OSACT Shared Task on Offensive Language Detection,,osact
2016.gwc-1.57,"['Data Management and Generation', 'Bilingual Lexicon Induction (BLI)', 'Low-resource Languages', 'Cross-lingual Application', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"This paper reports the work of creating bilingual mappings in English for certain synsets of Hindi wordnet, the need for doing this, the methods adopted and the tools created for the task. Hindi wordnet, which forms the foundation for other Indian language wordnets, has been linked to the English WordNet. To maximize linkages, an important strategy of using direct and hypernymy linkages has been followed. However, the hypernymy linkages were found to be inadequate in certain cases and posed a challenge due to sense granularity of language. Thus, the idea of creating bilingual mappings was adopted as a solution. A bilingual mapping means a linkage between a concept in two different languages, with the help of translation and/or transliteration. Such mappings retain meaningful representations, while capturing semantic similarity at the same time. This has also proven to be a great enhancement of Hindi wordnet and can be a crucial resource for multilingual applications in natural language processing, including machine translation and cross language information retrieval.",https://aclanthology.org/2016.gwc-1.57,Global Wordnet Association,2016,27--30 January,Proceedings of the 8th Global WordNet Conference (GWC),"Singh, Meghna  and
Shukla, Rajita  and
Saraswati, Jaya  and
Kashyap, Laxmi  and
Kanojia, Diptesh  and
Bhattacharyya, Pushpak",Mapping it differently: A solution to the linking challenges,,gwc
2016.lilt-14.6,"['Data Management and Generation', 'Classification Applications']",['Data Preparation'],['Annotation Processes'],"Modal auxiliaries have different readings, depending on the context in which they occur Kratzer, 1981 . Several projects have attempted to classify uses of modal auxiliaries in corpora according to their reading using supervised machine learning techniques e.g., Rubinstein et al., 2013 , Ruppenhofer & Rehbein, 2012 . In each study, traditional taxonomic labels, such as 'epistemic' and 'deontic' are used by human annotators to label instances of modal auxiliaries in a corpus. In order to achieve higher agreement among annotators, results in these previous studies are reported after collapsing some of the initial categories. The results show that human annotators have fairly good agreement on some of the categories, such as whether or not a use is epistemic, but poor agreement on others. They also show that annotators agree more on modals such as might than on modals such as could. In this study, we used traditional taxonomic categories on sentences containing modal auxiliary verbs that were randomly extracted from the English Gigaword 4 th edition corpus Parker et al., 2009 . The lowest inner-annotator agreement using traditional taxonomic labels occurred with uses of could, with raw agreements of 42% − 48% κ = 0.196 − 0.259, compared to might, for instance, with raw agreement of 98%. In response to the low numbers, rather than collapsing traditional categories, we tried a new method of classifying uses of could with respect to where the reading situates the eventuality being described relative to the speech time. For example, the sentence 'Jess could swim.'",https://aclanthology.org/2016.lilt-14.6,CSLI Publications,2016,sept,"Linguistic Issues in Language Technology, Volume 14, 2016 - Modality: Logic, Semantics, Annotation, and Machine Learning","Moon, Lori  and
Kirvaitis, Patricija  and
Madden, Noreen",Selective Annotation of Modal Readings: Delving into the Difficult Data,,lilt
2020.onion-1.2,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"We conducted preliminary comparison of human-robot HR interaction with human-human HH interaction conducted in English and in Japanese. As the result, body gestures increased in HR, while hand and head gestures decreased in HR. Concerning hand gesture, they were composed of more diverse and complex forms, trajectories and functions in HH than in HR. Moreover, English speakers produced 6 times more hand gestures than Japanese speakers in HH. Regarding head gesture, even though there was no difference in the frequency of head gestures between English speakers and Japanese speakers in HH, Japanese speakers produced slightly more nodding during the robot's speaking than English speakers in HR. Furthermore, positions of nod were different depending on the language. Concerning body gesture, participants produced body gestures mostly to regulate appropriate distance with the robot in HR. Additionally, English speakers produced slightly more body gestures than Japanese speakers.",https://aclanthology.org/2020.onion-1.2,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Mori, Taiga  and
Jokinen, Kristiina  and
Den, Yasuharu",Analysis of Body Behaviours in Human-Human and Human-Robot Interactions,,onion
2019.gwc-1.8,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Analysis'],,"The paper presents an effort on transferability of noun -verb and nounadjective derivative and semantic relations to noun -noun relations. The approach relies on information from semantic classes and existing inter-POS derivative and morphosemantic relations between noun and verb, and noun and adjective synsets. We have added semantic relations between nouns in WordNet that are indirectly linked via verbs and adjectives. Observations on the combination between the relations and semantic classes of nouns they link, may facilitate further efforts in assigning semantic properties to nouns pointing to their abilities to participate in predicate-argument structures.",https://aclanthology.org/2019.gwc-1.8,Global Wordnet Association,2019,July,Proceedings of the 10th Global Wordnet Conference,"Dimitrova, Tsvetana  and
Stefanova, Valentina",On Hidden Semantic Relations between Nouns in WordNet,,gwc
2021.bionlp-1.5,"['Domain-specific NLP', 'Information Retrieval', 'Knowledge Representation and Reasoning']",['Medical and Clinical NLP'],,"We explore whether state-of-the-art BERT models encode sufficient domain knowledge to correctly perform domain-specific inference. Although BERT implementations such as BioBERT are better at domain-based reasoning than those trained on general-domain corpora, there is still a wide margin compared to human performance on these tasks. To bridge this gap, we explore whether supplementing textual domain knowledge in the medical NLI task: a by further language model pretraining on the medical domain corpora, b by means of lexical match algorithms such as the BM25 algorithm, c by supplementing lexical retrieval with dependency relations, or d by using a trained retriever module, can push this performance closer to that of humans. We do not find any significant difference between knowledge supplemented classification as opposed to the baseline BERT models, however. This is contrary to the results for evidence retrieval on other tasks such as open domain question answering QA. By examining the retrieval output, we show that the methods fail due to unreliable knowledge retrieval for complex domain-specific reasoning. We conclude that the task of unsupervised text retrieval to bridge the gap in existing information to facilitate inference is more complex than what the state-of-the-art methods can solve, and warrants extensive research in the future.",https://aclanthology.org/2021.bionlp-1.5,Association for Computational Linguistics,2021,June,Proceedings of the 20th Workshop on Biomedical Language Processing,"Sushil, Madhumita  and
Suster, Simon  and
Daelemans, Walter",Are we there yet? Exploring clinical domain knowledge of BERT models,10.18653/v1/2021.bionlp-1.5,bionlp
2020.onion-1.3,"['Classification Applications', 'Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Image and Video Processing']",['Multimodal Learning'],,"This paper presents an approach to automatic head movement detection and classification in data from a corpus of video-recorded face-toface conversations in Danish involving 12 different speakers. A number of classifiers were trained with different combinations of visual, acoustic and word features and tested in a leave-one-out cross validation scenario. The visual movement features were extracted from the raw video data using OpenPose, the acoustic ones from the sound files using Praat, and the word features from the transcriptions. The best results were obtained by a Multilayer Perceptron classifier, which reached an average 0.68 F1 score across the 12 speakers for head movement detection, and 0.40 for head movement classification given four different classes. In both cases, the classifier outperformed a simple most frequent class baseline, a more advanced baseline only relying on velocity features, and linear classifiers using different combinations of features.",https://aclanthology.org/2020.onion-1.3,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Paggio, Patrizia  and
Agirrezabal, Manex  and
Jongejan, Bart  and
Navarretta, Costanza",Automatic Detection and Classification of Head Movements in Face-to-Face Conversations,,onion
2020.mwe-1.8,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['NLP for News and Media', 'Data Analysis']",['NLP for Social Media'],"We evaluate manually five lexical association measurements as regards the discovery of Modern Greek verb multiword expressions with two or more lexicalised components using mwetoolkit3 Ramisch et al., 2010. We use Twitter corpora and compare our findings with previous work on fiction corpora. The results of LL, MLE and T-score were found to overlap significantly in both the fiction and the Twitter corpora, while the results of PMI and Dice do not. We find that MWEs with two lexicalised components are more frequent in Twitter than in fiction corpora and that lean syntactic patterns help retrieve them more efficiently than richer ones. Our work i supports the enrichment of the lexicographical database for Modern Greek MWEs 'IDION' Markantonatou et al., 2019 and ii highlights aspects of the usage of five association measurements on specific text genres for best MWE discovery results.",https://aclanthology.org/2020.mwe-1.8,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Stamou, Vivian  and
Xylogianni, Artemis  and
Malli, Marilena  and
Takorou, Penny  and
Markantonatou, Stella",VMWE discovery: a comparative analysis between Literature and Twitter Corpora,,mwe
2020.cogalex-1.5,"['Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application', 'Multilingual NLP', 'Model Architectures']",['Data Preparation'],,"The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages systems were evaluated in a monolingual fashion and the other one proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/ cogalexvisharedtask/.",https://aclanthology.org/2020.cogalex-1.5,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Xiang, Rong  and
Chersoni, Emmanuele  and
Iacoponi, Luca  and
Santus, Enrico",The CogALex Shared Task on Monolingual and Multilingual Identification of Semantic Relations,,cogalex
2020.lincr-1.2,"['Domain-specific NLP', 'Low-resource Languages']",['Medical and Clinical NLP'],,"Texts comprise a large part of visual information that we process every day, so one of the tasks of language science is to make them more accessible. However, often the text design process is focused on the font size, but not on its type; which might be crucial especially for the people with reading disabilities. The current paper represents a study on text accessibility and the first attempt to create a researchbased accessible font for Cyrillic letters. This resulted in the dyslexic-specific font, LexiaD. Its design rests on the reduction of interletter similarity of the Russian alphabet. In evaluation stage, dyslexic and non-dyslexic children were asked to read sentences from the Children version of the Russian Sentence Corpus. We tested the readability of LexiaD compared to PT Sans and PT Serif fonts. The results showed that all children had some advantage in letter feature extraction and information integration while reading in LexiaD, but lexical access was improved when sentences were rendered in PT Sans or PT Serif. Therefore, in several aspects, LexiaD proved to be faster to read and could be recommended to use by dyslexics who have visual deficiency or those who struggle with text understanding resulting in re-reading.",https://aclanthology.org/2020.lincr-1.2,European Language Resources Association,2020,May,Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources,"Alexeeva, Svetlana  and
Dobrego, Aleksandra  and
Zubov, Vladislav",Towards the First Dyslexic Font in Russian,,lincr
2020.textgraphs-1.4,"['Data Management and Generation', 'Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Graph Neural Networks (GNNs)', 'Data Preparation']",,"Graph-based semi-supervised learning is appealing when labels are scarce but large amounts of unlabeled data are available. These methods typically use a heuristic strategy to construct the graph based on some fixed data representation, independently of the available labels. In this paper, we propose to jointly learn a data representation and a graph from both labeled and unlabeled data such that i the learned representation indirectly encodes the label information injected into the graph, and ii the graph provides a smooth topology with respect to the transformed data. Plugging the resulting graph and representation into existing graph-based semi-supervised learning algorithms like label spreading and graph convolutional networks, we show that our approach outperforms standard graph construction methods on both synthetic data and real datasets.",https://aclanthology.org/2020.textgraphs-1.4,Association for Computational Linguistics,2020,December,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),"Vargas-Vieyra, Mariana  and
Bellet, Aur{\'e}lien  and
Denis, Pascal",Joint Learning of the Graph and the Data Representation for Graph-Based Semi-Supervised Learning,10.18653/v1/2020.textgraphs-1.4,textgraphs
2020.sigmorphon-1.25,"['Information Retrieval', 'Audio Generation and Processing', 'Low-resource Languages', 'Text Generation']",,,"We investigate the problem of searching for a lexeme-set in speech by searching for its inflectional variants. Experimental results indicate how lexeme-set search performance changes with the number of hypothesized inflections, while ablation experiments highlight the relative importance of different components in the lexeme-set search pipeline and the value of using curated inflectional paradigms. We provide a recipe and evaluation set for the community to use as an extrinsic measure of the performance of inflection generation approaches.",https://aclanthology.org/2020.sigmorphon-1.25,Association for Computational Linguistics,2020,July,"Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Adams, Oliver  and
Wiesner, Matthew  and
Trmal, Jan  and
Nicolai, Garrett  and
Yarowsky, David",Induced Inflection-Set Keyword Search in Speech,10.18653/v1/2020.sigmorphon-1.25,sigmorphon
2021.conll-1.43,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection', 'Data Analysis']","['NLP for Social Media', 'Annotation Processes']","As users in online communities suffer from severe side effects of abusive language, many researchers attempted to detect abusive texts from social media, presenting several datasets for such detection. However, none of them contain both comprehensive labels and contextual information, which are essential for thoroughly detecting all kinds of abusiveness from texts, since datasets with such fine-grained features demand a significant amount of annotations, leading to much increased complexity. In this paper, we propose a Comprehensive Abusiveness Detection Dataset CADD, collected from the English Reddit posts, with multifaceted labels and contexts. Our dataset is annotated hierarchically for an efficient annotation through crowdsourcing on a large-scale. We also empirically explore the characteristics of our dataset and provide a detailed analysis for novel insights. The results of our experiments with strong pre-trained natural language understanding models on our dataset show that our dataset gives rise to meaningful performance, assuring its practicality for abusive language detection.",https://aclanthology.org/2021.conll-1.43,Association for Computational Linguistics,2021,November,Proceedings of the 25th Conference on Computational Natural Language Learning,"Song, Hoyun  and
Ryu, Soo Hyun  and
Lee, Huije  and
Park, Jong",A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit,10.18653/v1/2021.conll-1.43,conll
2021.depling-1.3,"['Parsing', 'Data Management and Generation', 'Multilingual NLP']","['Data Preparation', 'Semantic Parsing', 'Data Analysis', 'Syntactic Parsing']","['Annotation Processes', 'Dependency Parsing']","We discuss the role of enhanced Universal Dependencies E-UD in the task of deriving semantic predicate-argument structures from UD treebanks in a universal, non-language-specific way. We consider the usefulness of three kinds of E-UD annotation controllers of xcomps, propagation of outgoing dependencies in coordinations, and coreference in relative clauses and assess some heuristics for automatically adding such enhancements. We conclude that one large obstacle both for deriving predicate-argument structures from UD treebanks and for the automatic enhancement of basic UD treebanks is the fact that UD does not represent empty elements such as pro-dropped arguments, and we suggest that devoting effort to this would often provide a better return on investment than spending resources on improving or adding E-UD annotations. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 The relations should also be labelled, giving rise to a task of translating UD grammatical functions to appropriate semantic roles, but we do not consider this task here. Also, note that UD annotation does not allow us to identify eventualities introduced by non-verbal predicates e.g. action nouns, so we ignore those in this paper. 2  The TuDeT treebanks merely copy the basic dependencies over into the E-UD, while the Akkadian treebank only contains a single E-UD edge. 3 There are many existing systems for augmenting basic UD dependency trees, and several whose effectiveness has been reported in the literature ",https://aclanthology.org/2021.depling-1.3,Association for Computational Linguistics,2021,December,"Proceedings of the Sixth International Conference on Dependency Linguistics (Depling, SyntaxFest 2021)","Findlay, Jamie Y.  and
Haug, Dag T. T.",How useful are Enhanced Universal Dependencies for semantic interpretation?,,depling
2021.conll-1.18,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Machine Translation (MT)']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"This work describes an analysis of interannotator disagreements in human evaluation of machine translation output. The errors in the analysed texts were marked by multiple annotators under guidance of different quality criteria: adequacy, comprehension, and an unspecified generic mixture of adequacy and fluency. Our results show that different criteria result in different disagreements, and indicate that a clear definition of quality criterion can improve the inter-annotator agreement. Furthermore, our results show that for certain linguistic phenomena which are not limited to one or two words such as word ambiguity or gender but span over several words or even entire phrases such as negation or relative clause, disagreements do not necessarily represent ""errors"" or ""noise"" but are rather inherent to the evaluation process. On the other hand, for some other phenomena such as omission or verb forms agreement can be easily improved by providing more precise and detailed instructions to the evaluators.",https://aclanthology.org/2021.conll-1.18,Association for Computational Linguistics,2021,November,Proceedings of the 25th Conference on Computational Natural Language Learning,"Popovi{\'c}, Maja",Agree to Disagree: Analysis of Inter-Annotator Disagreements in Human Evaluation of Machine Translation Output,10.18653/v1/2021.conll-1.18,conll
2020.semeval-1.38,"['Embeddings', 'Multilingual NLP', 'Model Architectures', 'Low-resource Languages']",['Word Embeddings'],,"Natural Language Processing NLP has been widely used in the semantic analysis in recent years. Our paper mainly discusses a methodology to analyze the effect that context has on human perception of similar words, which is the third task of SemEval 2020. We apply several methods in calculating the distance between two embedding vector generated by Bidirectional Encoder Representation from Transformer BERT. Our team will go won the 1st place in Finnish language track of subtask1, the second place in English track of subtask1.",https://aclanthology.org/2020.semeval-1.38,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Bao, Wei  and
Che, Hongshu  and
Zhang, Jiandong",Will\_Go at SemEval-2020 Task 3: An Accurate Model for Predicting the Graded Effect of Context in Word Similarity Based on BERT,10.18653/v1/2020.semeval-1.38,semeval
2020.blackboxnlp-1.21,['Model Architectures'],,,"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we finetuned 100 instances of BERT on the Multigenre Natural Language Inference MNLI dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor, accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.",https://aclanthology.org/2020.blackboxnlp-1.21,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"McCoy, R. Thomas  and
Min, Junghyun  and
Linzen, Tal",BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance,10.18653/v1/2020.blackboxnlp-1.21,blackboxnlp
2020.clinicalnlp-1.20,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Medical and Clinical NLP']",,"Automated Medication Regimen MR extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spans for frequency, route and change, corresponding to medications discussed in the conversation. We first describe a unique dataset of annotated doctor-patient conversations and then present a weakly supervised model architecture that can perform span extraction using noisy classification data. The model utilizes an attention bottleneck inside a classification model to perform the extraction. We experiment with several variants of attention scoring and projection functions and propose a novel transformer-based attention scoring function TAScore. The proposed combination of TAScore and Fusedmax projection achieves a 10 point increase in Longest Common Substring F1 compared to the baseline of additive scoring plus softmax projection.",https://aclanthology.org/2020.clinicalnlp-1.20,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Patel, Dhruvesh  and
Konam, Sandeep  and
Prabhakar, Sai",Weakly Supervised Medication Regimen Extraction from Medical Conversations,10.18653/v1/2020.clinicalnlp-1.20,clinicalnlp
2021.americasnlp-1.22,"['Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Multilingual NLP', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Transfer Learning']",,"Peru is a multilingual country with a long history of contact between the indigenous languages and Spanish. Taking advantage of this context for machine translation is possible with multilingual approaches for learning both unsupervised subword segmentation and neural machine translation models. The study proposes the first multilingual translation models for four languages spoken in Peru: Aymara, Ashaninka, Quechua and Shipibo-Konibo, providing both many-to-Spanish and Spanishto-many models and outperforming pairwise baselines in most of them. The task exploited a large English-Spanish dataset for pretraining, monolingual texts with tagged backtranslation, and parallel corpora aligned with English. Finally, by fine-tuning the best models, we also assessed the out-of-domain capabilities in two evaluation datasets for Quechua and a new one for Shipibo-Konibo 1 .",https://aclanthology.org/2021.americasnlp-1.22,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Oncevay, Arturo","Peru is Multilingual, Its Machine Translation Should Be Too?",10.18653/v1/2021.americasnlp-1.22,americasnlp
2020.semeval-1.252,"['Domain-specific NLP', 'Classification Applications', 'Low-resource Languages', 'Multilingual NLP', 'Model Architectures']","['NLP for News and Media', 'Hate and Offensive Speech Detection', 'Transformer Models']",['NLP for Social Media'],"With today's proliferation of maliciously intended communication across all social media platforms, finding ways of effectively combating these messages grows increasingly important. We present our submission and results for SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media OffensEval 2020 where we participated in offensive tweet classification tasks in English, Arabic, Greek, Turkish and Danish. Our approach included classical machine learning architectures such as support vector machines and logistic regression combined in an ensemble with a multilingual transformer-based model XLM-R. The transformer model is trained on all languages combined in order to create a fully multilingual model which can leverage knowledge between languages. The machine learning model hyperparameters are fine-tuned and the statistically best performing ones included in the final ensemble. We further discuss the results of our model and see that our broad approach provides competitive but not task-winning performance. We also include an error analysis and potential improvements for future work.",https://aclanthology.org/2020.semeval-1.252,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Chapman, Kathryn  and
Bernhard, Johannes  and
Klakow, Dietrich",CoLi at UdS at SemEval-2020 Task 12: Offensive Tweet Detection with Ensembling,10.18653/v1/2020.semeval-1.252,semeval
2020.splu-1.7,"['Image and Video Processing', 'Data Management and Generation']",['Data Preparation'],,"The Touchdown dataset Chen et al., 2019 provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release Mirowski et al., 2019  to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation VLN and spatial description resolution SDR. We compare our model results to those given in Chen et al.  2019  and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.",https://aclanthology.org/2020.splu-1.7,Association for Computational Linguistics,2020,November,Proceedings of the Third International Workshop on Spatial Language Understanding,"Mehta, Harsh  and
Artzi, Yoav  and
Baldridge, Jason  and
Ie, Eugene  and
Mirowski, Piotr",Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View,10.18653/v1/2020.splu-1.7,splu
2020.finnlp-1.10,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages', 'Text Preprocessing']","['NLP for Finance', 'Data Augmentation', 'Text Segmentation']",,"This paper describes the method that we submitted to the FinSBD2-shared task in IJCAI-2020 to detect the sentence, list, and item boundaries and classify the items from noisy unstructured English and French financial texts. We used the spatial and semantic information of text to augment each tokenized word of text as a fixed-length sentence, and we labeled each word sentence as different boundary types. Then, we proposed the deep attention model based on word embedding to detect the sentence, list, and items boundaries in noisy English and French texts extracted from the financial documents and classified the item sentences into different item types. The experiment shows that the proposed method could be an effective solution to deal with the FinSBD2-shared task.",https://aclanthology.org/2020.finnlp-1.10,-,2020,5 January,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,"Tian, Ke  and
Chen, Hua  and
Yang, Jie","aiai at the FinSBD-2 Task: Sentence, list and Item Boundary Detection and Items classification of Financial Texts Using Data Augmentation and Attention",,finnlp
2020.parlaclarin-1.10,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']",['Data Analysis'],,"This paper addresses differences in the word use of two left-winged and two right-winged Danish parties, and how these differences, which reflect some of the basic stances of the parties, can be used to automatically identify the party of politicians from their speeches. In the first study, the most frequent and characteristic lemmas in the manifestos of the political parties as well as their language complexity are analysed. The analysis shows inter alia that the most frequently occurring lemmas in the manifestos reflect either the ideology or the position of the parties towards specific subjects, confirming for Danish preceding studies of English and German manifestos. Successively, we scaled our analysis applying NLP methods to the transcribed speeches by members of the same parties in the Parliament Hansards and trained machine learning algorithms in order to determine to what extent it is possible to predict the party of the politicians from the speeches. The speeches are a subset of the Danish Parliament corpus 2009-2017. The best results of the classification experiments gave a weighted F1-score of 0.57. These results are significantly better than the results obtained by the majority classifier weighted F1-score = 0.11 and by chance results. They show that the party of the politicians can be distinguished from their speeches in nearly 60% of the cases, even if they debate about the same subjects and thus often use the same terminology. In the future, we will include the subject of the speeches in the prediction experiments.",https://aclanthology.org/2020.parlaclarin-1.10,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Navarretta, Costanza  and
Haltrup Hansen, Dorte",Identifying Parties in Manifestos and Parliament Speeches,,parlaclarin
2020.iwltp-1.7,"['Multilingual NLP', 'Audio Generation and Processing', 'Machine Translation (MT)']",['Automatic Speech Recognition (ASR)'],,"This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far.",https://aclanthology.org/2020.iwltp-1.7,European Language Resources Association,2020,May,Proceedings of the 1st International Workshop on Language Technology Platforms,"Franceschini, Dario  and
Canton, Chiara  and
Simonini, Ivan  and
Schweinfurth, Armin  and
Glott, Adelheid  and
St{\""u}ker, Sebastian  and
Nguyen, Thai-Son  and
Schneider, Felix  and
Ha, Thanh-Le  and
Waibel, Alex  and
Haddow, Barry  and
Williams, Philip  and
Sennrich, Rico  and
Bojar, Ond{\v{r}}ej  and
Sagar, Sangeet  and
Mach{\'a}{\v{c}}ek, Dominik  and
Smr{\v{z}}, Otakar",Removing European Language Barriers with Innovative Machine Translation Technology,,iwltp
2020.crac-1.14,"['Data Management and Generation', 'Domain-specific NLP', 'Information Extraction']","['NLP for News and Media', 'Data Preparation', 'Coreference Resolution']","['NLP for Social Media', 'Annotation Processes']","Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event.",https://aclanthology.org/2020.crac-1.14,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference","Andy, Anietie  and
Callison-Burch, Chris  and
Wijaya, Derry Tanti",Resolving Pronouns in Twitter Streams: Context can Help!,,crac
2020.lt4hala-1.1,"['Data Management and Generation', 'Model Architectures', 'Low-resource Languages', 'Language Change Analysis']",['Data Analysis'],,"This paper introduces and evaluates a Bayesian mixture model that is designed for dating texts based on the distributions of linguistic features. The model is applied to the corpus of Vedic Sanskrit the historical structure of which is still unclear in many details. The evaluation concentrates on the interaction between time, genre and linguistic features, detecting those whose distributions are clearly coupled with the historical time. The evaluation also highlights the problems that arise when quantitative results need to be reconciled with philological insights.",https://aclanthology.org/2020.lt4hala-1.1,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Hellwig, Oliver",Dating and Stratifying a Historical Corpus with a Bayesian Mixture Model,,lt4hala
2020.sigdial-1.25,"['Data Management and Generation', 'Model Architectures', 'Discourse Analysis']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"During an interaction the tendency of speakers to change their speech production to make it more similar to their interlocutor's speech is called convergence. Convergence had been studied due to its relevance for cognitive models of communication as well as for dialogue system adaptation to the user. Convergence effects have been established on controlled data sets while tracking its dynamics on generic corpora has provided positive but more contrasted outcomes. We propose to enrich large conversational corpora with dialogue acts information and to use these acts as filters to create subsets of homogeneous conversational activity. Those subsets allow a more precise comparison between speakers' speech variables. We compare convergence on acoustic variables Energy, Pitch and Speech Rate measured on raw data sets, with human and automatically data sets labelled with dialog acts type. We found that such filtering helps in observing convergence suggesting that future studies should consider such high level dialogue activity types and the related NLP techniques as important tools for analyzing conversational interpersonal dynamics.",https://aclanthology.org/2020.sigdial-1.25,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Fuscone, Simone  and
Favre, Benoit  and
Pr{\'e}vot, Laurent",Filtering conversations through dialogue acts labels for improving corpus-based convergence studies,,sigdial
2020.finnlp-1.3,"['Evaluation Techniques', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']",['Data Preparation'],,"Regulators require most companies to publish yearly reports, describing their activities, results, future plans, and risk factors. Sometimes a risk factor can be omitted in a document, possiblyvoluntarily or not-misleading the readers. In this paper, we introduce a task for detecting omitted risk factors in Annual Reports. This new task requires to catch the risks mentions in multiple sentences, and to identify the ones that are specific to a sector or a period. To address it, we use a neural architecture to extract risk sentences from documents and cluster the risk factors from these sentences. Finally, we generate synthetic risk factor omissions and propose a metric to evaluate the omission detection method.",https://aclanthology.org/2020.finnlp-1.3,-,2020,5 January,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,"Masson, Corentin  and
Montariol, Syrielle",Detecting Omissions of Risk Factors in Company Annual Reports,,finnlp
2020.nlpcss-1.17,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['NLP for News and Media', 'Data Preparation']",,"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.",https://aclanthology.org/2020.nlpcss-1.17,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Gupta, Sarang  and
Nishu, Kumari",Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model,10.18653/v1/2020.nlpcss-1.17,nlpcss
2020.challengehml-1.1,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Transformer Models', 'Sentiment Analysis (SA)', 'Multimodal Learning', 'Multilabel Text Classification', 'Emotion Detection']",,"Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding TBJE for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source 1 .",https://aclanthology.org/2020.challengehml-1.1,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Delbrouck, Jean-Benoit  and
Tits, No{\'e}  and
Brousmiche, Mathilde  and
Dupont, St{\'e}phane",A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis,10.18653/v1/2020.challengehml-1.1,challengehml
2020.rail-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Data Augmentation']",,"The recent advances in Natural Language Processing have only been a boon for well represented languages, negating research in lesser known global languages. This is in part due to the availability of curated data and research resources. One of the current challenges concerning low-resourced languages are clear guidelines on the collection, curation and preparation of datasets for different use-cases. In this work, we take on the task of creating two datasets that are focused on news headlines i.e short text for Setswana and Sepedi and the creation of a news topic classification task from these datasets. In this study, we document our work, propose baselines for classification, and investigate an approach on data augmentation better suited to low-resourced languages in order to improve the performance of the classifiers.",https://aclanthology.org/2020.rail-1.3,European Language Resources Association (ELRA),2020,May,Proceedings of the first workshop on Resources for African Indigenous Languages,"Marivate, Vukosi  and
Sefara, Tshephisho  and
Chabalala, Vongani  and
Makhaya, Keamogetswe  and
Mokgonyane, Tumisho  and
Mokoena, Rethabile  and
Modupe, Abiodun","Investigating an Approach for Low Resource Language Dataset Creation, Curation and Classification: Setswana and Sepedi",,rail
2020.figlang-1.38,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['Sarcasm Detection', 'NLP for News and Media', 'Transformer Models']",['NLP for Social Media'],"We present a transformer-based sarcasm detection model that accounts for the context from the entire conversation thread for more robust predictions. Our model uses deep transformer layers to perform multi-head attentions among the target utterance and the relevant context in the thread. The context-aware models are evaluated on two datasets from social media, Twitter and Reddit, and show 3.1% and 7.0% improvements over their baselines. Our best models give the F1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively, becoming one of the highest performing systems among 36 participants in this shared task.",https://aclanthology.org/2020.figlang-1.38,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Dong, Xiangjue  and
Li, Changmao  and
Choi, Jinho D.",Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media,10.18653/v1/2020.figlang-1.38,figlang
2019.lilt-18.4,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Data Analysis', 'Syntactic Parsing']",,"This paper presents a case study of the use of the NINJAL Parsed Corpus of Modern Japanese NPCMJ for syntactic research. NPCMJ is the first phrase structure-based treebank for Japanese that is specifically designed for application in linguistic in addition to NLP research. After discussing some basic methodological issues pertaining to the use of treebanks for theoretical linguistics research, we introduce our case study on the status of the Coordinate Structure Constraint CSC in Japanese, showing that NPCMJ enables us to easily retrieve examples that support one of the key claims of Kubota and Lee 2015 : that the CSC should be viewed as a pragmatic, rather than a syntactic constraint. The corpus-based study we conducted moreover revealed a previously unnoticed tendency that was highly relevant for further clarifying the principles governing the empirical data in question. We conclude the paper by briefly discussing some further methodological issues brought up by our case study pertaining to the relationship between linguistic research and corpus development.",https://aclanthology.org/2019.lilt-18.4,CSLI Publications,2019,July,"Linguistic Issues in Language Technology, Volume 18, 2019 - Exploiting Parsed Corpora: Applications in Research, Pedagogy, and Processing","Kubota, Yusuke  and
Kubota, Ai",Probing the nature of an island constraint with a parsed corpus,,lilt
2020.parlaclarin-1.11,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Language Change Analysis']",['Data Analysis'],,"Most diachronic studies on either lexico-semantic change or political language usage are based on individual or structurally similar corpora. In this paper, we explore ways of studying the stability and changeability of lexical usage in political discourse across two corpora which are substantially different in structure and size. We present a case study focusing on lexical items associated with political parties in two diachronic corpora of Austrian German, namely a diachronic media corpus AMC and a corpus of parliamentary records ParlAT, and measure the cross-temporal stability of lexical usage over a period of 20 years. We conduct three sets of comparative analyses investigating a the stability of sets of lexical items associated with the three major political parties over time, b lexical similarity between parties, and c the similarity between the lexical choices in parliamentary speeches by members of the parties vis-à-vis the media's reporting on the parties. We employ time series modeling using generalized additive models GAMs to compare the lexical similarities and differences between parties within and across corpora. The results show that changes observed in these measures can be meaningfully related to political events during that time.",https://aclanthology.org/2020.parlaclarin-1.11,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Hofmann, Klaus  and
Marakasova, Anna  and
Baumann, Andreas  and
Neidhardt, Julia  and
Wissik, Tanja",Comparing Lexical Usage in Political Discourse across Diachronic Corpora,,parlaclarin
2020.emnlp-main.376,"['Biases in NLP', 'Data Management and Generation']",['Data Preparation'],,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures e.g. GPT-2 tend to out-perform recurrent architectures e.g. LSTMs even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.",https://aclanthology.org/2020.emnlp-main.376,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Hawkins, Robert  and
Yamakoshi, Takateru  and
Griffiths, Thomas  and
Goldberg, Adele",Investigating representations of verb bias in neural language models,10.18653/v1/2020.emnlp-main.376,emnlp
2020.framenet-1.12,"['Domain-specific NLP', 'Data Management and Generation', 'Parsing', 'Low-resource Languages', 'Knowledge Representation and Reasoning', 'Multilingual NLP']","['Data Preparation', 'Semantic Parsing']",,"The methodology developed within the FrameNet project is being used to compile resources in an increasing number of specialized fields of knowledge. The methodology along with the theoretical principles on which it is based, i.e. Frame Semantics, are especially appealing as they allow domain-specific resources to account for the conceptual background of specialized knowledge and to explain the linguistic properties of terms against this background. This paper presents a methodology for building a multilingual resource that accounts for terms of the environment. After listing some lexical and conceptual differences that need to be managed in such a resource, we explain how the FrameNet methodology is adapted for describing terms in different languages. We first applied our methodology to French and then extended it to English. Extensions to Spanish, Portuguese and Chinese were made more recently. Up to now, we have defined 190 frames: 112 frames are new; 38 are used as such; and 40 are slightly different a different number of obligatory participants; a significant alternation, etc. when compared to Berkeley FrameNet.",https://aclanthology.org/2020.framenet-1.12,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","L{'} Homme, Marie-Claude  and
Robichaud, Beno{\^\i}t  and
Subirats, Carlos",Building Multilingual Specialized Resources Based on FrameNet: Application to the Field of the Environment,,framenet
2020.inlg-1.27,"['Machine Translation (MT)', 'Text Generation']",,,"Massive digital disinformation is one of the main risks of modern society. Hundreds of models and linguistic analyses have been done to compare and contrast misleading and credible content online. However, most models do not remove the confounding factor of a topic or narrative when training, so the resulting models learn a clear topical separation for misleading versus credible content. We study the feasibility of using two strategies to disentangle the topic bias from the models to understand and explicitly measure linguistic and stylistic properties of content from misleading versus credible content. First, we develop conditional generative models to create news content that is characteristic of different credibility levels. We perform multi-dimensional evaluation of model performance on mimicking both the style and linguistic differences that distinguish news of different credibility using machine translation metrics and classification models. We show that even though generative models are able to imitate both the style and language of the original content, additional conditioning on both the news category and the topic leads to reduced performance. In a second approach, we perform deception style ""transfer"" by translating deceptive content into the style of credible content and vice versa. Extending earlier studies, we demonstrate that, when conditioned on a topic, deceptive content is shorter, less readable, more biased, and more subjective than credible content, and transferring the style from deceptive to credible content is more challenging than the opposite direction.",https://aclanthology.org/2020.inlg-1.27,Association for Computational Linguistics,2020,December,Proceedings of the 13th International Conference on Natural Language Generation,"Saldanha, Emily  and
Garimella, Aparna  and
Volkova, Svitlana",Understanding and Explicitly Measuring Linguistic and Stylistic Properties of Deception via Generation and Translation,,inlg
2020.cogalex-1.14,"['Low-resource Languages', 'Machine Translation (MT)']",,,"Existing dictionaries may help collocation translation by suggesting associated words in the form of collocations, thesaurus, and example sentences. We propose to enhance them with taskdriven word associations, illustrating the need by a few scenarios and outlining a possible approach based on word embedding. An example is given, using pre-trained word embedding, while more extensive investigation with more refined methods and resources is underway.",https://aclanthology.org/2020.cogalex-1.14,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Kwong, Oi Yee",Translating Collocations: The Need for Task-driven Word Associations,,cogalex
2021.adaptnlp-1.21,"['Parsing', 'Embeddings', 'Low-resource Languages', 'Learning Paradigms', 'Cross-lingual Application', 'Multilingual NLP']","['Word Embeddings', 'Unsupervised Learning', 'Syntactic Parsing']",['Dependency Parsing'],"Linear embedding transformation has been shown to be effective for zero-shot crosslingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.",https://aclanthology.org/2021.adaptnlp-1.21,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Xu, Haoran  and
Koehn, Philipp",Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation,,adaptnlp
2020.findings-emnlp.140,['Dialogue Systems'],['Response Generation'],,"Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging task with pre-trained language models that have brought breakthrough to various natural language tasks. To this end, we introduce a KL loss and a style classifier to the fine-tuning step in order to steer response generation towards the target style in both a word-level and a sentence-level. Comprehensive empirical studies with two public datasets indicate that our model can significantly outperform stateof-the-art methods in terms of both style consistency and contextual coherence.",https://aclanthology.org/2020.findings-emnlp.140,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Yang, Ze  and
Wu, Wei  and
Xu, Can  and
Liang, Xinnian  and
Bai, Jiaqi  and
Wang, Liran  and
Wang, Wei  and
Li, Zhoujun",StyleDGPT: Stylized Response Generation with Pre-trained Language Models,10.18653/v1/2020.findings-emnlp.140,findings
2020.signlang-1.33,"['Image and Video Processing', 'Data Management and Generation', 'Low-resource Languages']","['Sign Language and Fingerspelling Recognition', 'Data Preparation']",['Annotation Processes'],"Representation of linguistic data is an issue of utmost importance when developing language resources, but the lack of a standard written form in sign languages presents a challenge. Different notation systems exist, but only SignWriting seems to have some use in the native signer community. It is, however, a difficult system to use computationally, not based on a linear sequence of characters. We present the project ""VisSE"", which aims to develop tools for the effective use of SignWriting in the computer. The first of these is an application which uses computer vision to interpret SignWriting, understanding the meaning of new or existing transcriptions, or even hand-written images. Two additional tools will be able to consume the result of this recognizer: first, a textual description of the features of the transcription will make it understandable for non-signers. Second, a three-dimensional avatar will be able to reproduce the configurations and movements contained within the transcription, making it understandable for signers even if not familiar with SignWriting. Additionally, the project will result in a corpus of annotated SignWriting data which will also be of use to the computational linguistics community.",https://aclanthology.org/2020.signlang-1.33,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","Sevilla, Antonio F. G.  and
D{\'\i}az Esteban, Alberto  and
Lahoz-Bengoechea, Jos{\'e} Mar{\'\i}a",Tools for the Use of SignWriting as a Language Resource,,signlang
2020.eamt-1.55,['Machine Translation (MT)'],"['Neural MT (NMT)', 'Statistical MT (SMT)']",,"In this paper the MTUOC project, aiming to provide an easy integration of neural and statistical machine translation systems, is presented. Almost all the required software to train and use neural and statistical MT systems is released under free licences. However, their use is not always easy and intuitive and medium-high specialized skills are required. MTUOC project provides simplified scripts for preprocessing and training MT systems, and a server and client for easy use of the trained systems. The server is compatible with popular CAT tools for a seamless integration. The project also distributes some free engines.",https://aclanthology.org/2020.eamt-1.55,European Association for Machine Translation,2020,November,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,"Oliver, Antoni",MTUOC: easy and free integration of NMT systems in professional translation environments,,eamt
2020.nlpbt-1.2,"['Audio Generation and Processing', 'Learning Paradigms']","['Multimodal Learning', 'Automatic Speech Recognition (ASR)']",,"Visual context has been shown to be useful for automatic speech recognition ASR systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called Rand-WordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.",https://aclanthology.org/2020.nlpbt-1.2,Association for Computational Linguistics,2020,November,Proceedings of the First International Workshop on Natural Language Processing Beyond Text,"Srinivasan, Tejas  and
Sanabria, Ramon  and
Metze, Florian  and
Elliott, Desmond",Multimodal Speech Recognition with Unstructured Audio Masking,10.18653/v1/2020.nlpbt-1.2,nlpbt
2020.framenet-1.8,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Analysis'],,"This paper presents the first investigation on using semantic frames to assess text difficulty. Based on Mandarin VerbNet, a verbal semantic database that adopts a frame-based approach, we examine usage patterns of ten verbs in a corpus of graded Chinese texts. We identify a number of characteristics in texts at advanced grades: more frequent use of non-core frame elements; more frequent omission of some core frame elements; increased preference for noun phrases rather than clauses as verb arguments; and more frequent metaphoric usage. These characteristics can potentially be useful for automatic prediction of text readability.",https://aclanthology.org/2020.framenet-1.8,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","Lee, John  and
Liu, Meichun  and
Cai, Tianyuan",Using Verb Frames for Text Difficulty Assessment,,framenet
2020.coling-main.477,"['Domain-specific NLP', 'Data Management and Generation', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Misinformation Detection', 'Data Analysis']",['Fake News Detection'],"Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.",https://aclanthology.org/2020.coling-main.477,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Del Tredici, Marco  and
Fern{\'a}ndez, Raquel",Words are the Window to the Soul: Language-based User Representations for Fake News Detection,10.18653/v1/2020.coling-main.477,coling
2021.adaptnlp-1.23,"['Evaluation Techniques', 'Domain-specific NLP']",['NLP for News and Media'],['NLP for Social Media'],"The robustness of pretrained language models PLMs is generally measured using performance drops on two or more domains. However, we do not yet understand the inherent robustness achieved by contributions from different layers of a PLM. We systematically analyze the robustness of these representations layer by layer from two perspectives. First, we measure the robustness of representations by using domain divergence between two domains. We find that i Domain variance increases from the lower to the upper layers for vanilla PLMs; ii Models continuously pretrained on domain-specific data DAPT Gururangan et al., 2020  exhibit more variance than their pretrained PLM counterparts; and that iii Distilled models e.g.,DistilBERT also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain.",https://aclanthology.org/2021.adaptnlp-1.23,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Ramesh Kashyap, Abhinav  and
Mehnaz, Laiba  and
Malik, Bhavitvya  and
Waheed, Abdul  and
Hazarika, Devamanyu  and
Kan, Min-Yen  and
Shah, Rajiv Ratn","Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer",,adaptnlp
2020.framenet-1.10,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"The Emirati Arabic FrameNet EAFN project aims to initiate a FrameNet for Emirati Arabic, utilizing the Emirati Arabic Corpus. The goal is to create a resource comparable to the initial stages of the Berkeley FrameNet. The project is divided into manual and automatic tracks, based on the predominant techniques being used to collect frames in each track. Work on the EAFN is progressing, and we here report on initial results for annotations and evaluation. The EAFN project aims to provide a general semantic resource for the Arabic language, sure to be of interest to researchers from general linguistics to natural language processing. As we report here, the EAFN is well on target for the first release of data in the coming year.",https://aclanthology.org/2020.framenet-1.10,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","Gargett, Andrew  and
Leung, Tommi",Building the Emirati Arabic FrameNet,,framenet
2020.wnut-1.75,"['Model Architectures', 'Classification Applications']","['Recurrent Neural Networks (RNNs)', 'Transformer Models']",['Long Short-Term Memory (LSTM) Models'],"In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of the output of the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.",https://aclanthology.org/2020.wnut-1.75,Association for Computational Linguistics,2020,November,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),"H{\""u}rriyeto{\u{g}}lu, Ali  and
Safaya, Ali  and
Mutlu, Osman  and
Oostdijk, Nelleke  and
Y{\""o}r{\""u}k, Erdem",COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules,10.18653/v1/2020.wnut-1.75,wnut
2020.figlang-1.9,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['Sarcasm Detection', 'NLP for News and Media', 'Transformer Models', 'Sentiment Analysis (SA)']","['NLP for Social Media', 'Aspect-Based SA (ABSA)']","Sarcasm is a type of figurative language broadly adopted in social media and daily conversations. The sarcasm can ultimately alter the meaning of the sentence, which makes the opinion analysis process error-prone. In this paper, we propose to employ bidirectional encoder representations transformers BERT, and aspect-based sentiment analysis approaches in order to extract the relation between context dialogue sequence and response and determine whether or not the response is sarcastic. The best performing method of ours obtains an F1 score of 0.73 on the Twitter dataset and 0.734 over the Reddit dataset at the second workshop on figurative language processing Shared Task 2020.",https://aclanthology.org/2020.figlang-1.9,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Shangipour ataei, Taha  and
Javdan, Soroush  and
Minaei-Bidgoli, Behrouz",Applying Transformers and Aspect-based Sentiment Analysis approaches on Sarcasm Detection,10.18653/v1/2020.figlang-1.9,figlang
2020.parlaclarin-1.13,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['NLP for the Legal Domain', 'Data Analysis']",,"The TAPS corpus makes it possible to share a large volume of French parliamentary data. The TEI-compliant approach behind its design choices facilitates the publishing and the interoperability of data, but also the implementation of exploratory data analysis techniques in order to process institutional or political discourse. We demonstrate its application to the debates occurred in the context of a specific legislative process, which generated a strong opposition.",https://aclanthology.org/2020.parlaclarin-1.13,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Diwersy, Sascha  and
Luxardo, Giancarlo",Querying a large annotated corpus of parliamentary debates,,parlaclarin
2020.acl-main.330,"['Data Management and Generation', 'Classification Applications', 'Question Answering (QA)', 'Low-resource Languages', 'Automatic Text Summarization']",['Data Preparation'],,"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization. MAT-INF contains 1.07 million question-answer pairs with human-labeled categories and usergenerated question descriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MAT-INF. 1",https://aclanthology.org/2020.acl-main.330,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Xu, Canwen  and
Pei, Jiaxin  and
Wu, Hongtao  and
Liu, Yiyu  and
Li, Chenliang","MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization",10.18653/v1/2020.acl-main.330,acl
2020.lrec-1.274,"['Domain-specific NLP', 'Data Management and Generation', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Medical and Clinical NLP']",['Annotation Processes'],"This paper proposes a representation framework for encoding spatial language in radiology based on frame semantics. The framework is adopted from the existing SpatialNet representation in the general domain with the aim to generate more accurate representations of spatial language used by radiologists. We describe Rad-SpatialNet in detail along with illustrating the importance of incorporating domain knowledge in understanding the varied linguistic expressions involved in different radiological spatial relations. This work also constructs a corpus of 400 radiology reports of three examination types chest X-rays, brain MRIs, and babygrams annotated with fine-grained contextual information according to this schema. Spatial trigger expressions and elements corresponding to a spatial frame are annotated. We apply BERT-based models BERTBASE and BERTLARGE to first extract the trigger terms lexical units for a spatial frame and then to identify the related frame elements. The results of BERTLARGE are decent, with F1 of 77.89 for spatial trigger extraction and an overall F1 of 81.61 and 66.25 across all frame elements using gold and predicted spatial triggers respectively. This frame-based resource can be used to develop and evaluate more advanced natural language processing NLP methods for extracting fine-grained spatial information from radiology text in the future.",https://aclanthology.org/2020.lrec-1.274,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Datta, Surabhi  and
Ulinski, Morgan  and
Godfrey-Stovall, Jordan  and
Khanpara, Shekhar  and
Riascos-Castaneda, Roy F.  and
Roberts, Kirk",Rad-SpatialNet: A Frame-based Resource for Fine-Grained Spatial Relations in Radiology Reports,,lrec
2019.lilt-18.6,"['Domain-specific NLP', 'Data Management and Generation']",,,"The principal barrier to the uptake of technologies in schools is not technological, but social and political. Teachers must be convinced of the pedagogical benefits of a particular curriculum before they will agree to learn the means to teach it. The teaching of formal grammar to first language students in schools is no exception to this rule. Over the last three decades, most schools in England have been legally required to teach grammatical subject knowledge, i.e. linguistic knowledge of grammar terms and structure, to children age five and upwards as part of the national curriculum in English. A mandatory set of curriculum specifications for England and Wales was published in 2014, and elsewhere similar requirements were imposed. However, few current English school teachers were taught grammar themselves, and the dominant view has long been in favour of 'real books' rather than the teaching of a formal grammar. English grammar teaching thus faces multiple challenges: to convince teachers of the value of grammar in their own teaching, to teach the teachers the knowledge they need, and to develop relevant resources to use in the classroom. Alongside subject knowledge, teachers need pedagogical knowledgehow to teach grammar effectively and how to integrate this teaching into other kinds of language learning. The paper introduces the Englicious 1 web platform for schools, and The Englicious project was funded by the UK research councils AHRC 1",https://aclanthology.org/2019.lilt-18.6,CSLI Publications,2019,July,"Linguistic Issues in Language Technology, Volume 18, 2019 - Exploiting Parsed Corpora: Applications in Research, Pedagogy, and Processing","Wallis, Sean  and
Cushing, Ian  and
Aarts, Bas",Exploiting parsed corpora in grammar teaching,,lilt
2020.msr-1.3,"['Data Management and Generation', 'Text Preprocessing']",['Data Augmentation'],,"In this paper, we describe the ADAPT submission to the Surface Realization Shared Task 2020. We present a neural-based system trained on the English Web Treebank and an augmented dataset, automatically created from existing text corpora.",https://aclanthology.org/2020.msr-1.3,Association for Computational Linguistics,2020,December,Proceedings of the Third Workshop on Multilingual Surface Realisation,"Elder, Henry",ADAPT at SR'20: How Preprocessing and Data Augmentation Help to Improve Surface Realization,,msr
2020.sigdial-1.26,"['Domain-specific NLP', 'Data Management and Generation', 'Dialogue Systems', 'Discourse Analysis']",['Data Analysis'],,"The present study aims to examine the prevalent notion that people entrain to the vocabulary of a dialogue system. Although previous research shows that people will replace their choice of words with simple substitutes, studies using more challenging substitutions are sparse. In this paper, we investigate whether people adapt their speech to the vocabulary of a dialogue system when the system's suggested words are not direct synonyms. 32 participants played a geographythemed game with a remote-controlled agent and were primed by referencing strategies rather than individual terms introduced in follow-up questions. Our results suggest that context-appropriate substitutes support convergence and that the convergence has a lasting effect within a dialogue session if the system's wording is more consistent with the norms of the domain than the original wording of the speaker.",https://aclanthology.org/2020.sigdial-1.26,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Bergqvist, Amanda  and
Manuvinakurike, Ramesh  and
Karkada, Deepthi  and
Paetzel, Maike",Nontrivial Lexical Convergence in a Geography-Themed Game,,sigdial
2020.cllrd-1.4,"['Information Extraction', 'Data Management and Generation']","['Data Preparation', 'Anaphora Resolution']",['Annotation Processes'],"Crowdsourcing approaches provide a difficult design challenge for developers. There is a trade-off between the efficiency of the task to be done and the reward given to the user for participating, whether it be altruism, social enhancement, entertainment or money. This paper explores how crowdsourcing and citizen science systems collect data and complete tasks, illustrated by a case study from the online language game-with-a-purpose Phrase Detectives. The game was originally developed to be a constrained interface to prevent player collusion, but subsequently benefited from posthoc analysis of over 76k unconstrained inputs from users. Understanding the interface design and task deconstruction are critical for enabling users to participate in such systems and the paper concludes with a discussion of the idea that social networks can be viewed as form of citizen science platform with both constrained and unconstrained inputs making for a highly complex dataset.",https://aclanthology.org/2020.cllrd-1.4,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',"Chamberlain, Jon  and
Kruschwitz, Udo  and
Poesio, Massimo",Speaking Outside the Box: Exploring the Benefits of Unconstrained Input in Crowdsourcing and Citizen Science Platforms,,cllrd
2019.rocling-1.28,"['Domain-specific NLP', 'Data Management and Generation', 'Audio Generation and Processing']",['Data Preparation'],,"Human perception on the singing voice differs with the factors of the singing voice and the subjects. On one hand, the background knowledge influences the understanding of voice for each subject. On the other hand, the difference of the voices presented to the subjects also affects the perception. In this paper, we discuss two factors reflecting on the similarity before and after singing voice conversion: prosodic features and subjects' familiarity to the singers. Three experiments were conducted. The first experiment tested the subjects' ability to identify the singer. The second experiment synthesized the singing voice with different singers' prosodic features, and let the subjects score the similarity. The third experiment presented timbre-converted singing voice with different combinations of prosodic features from two singers to the subjects for them to judge the similarity to the target singer. The results show that, first, the number of prosodic features contained in the synthesized voice is positively correlated with the scores in identification and similarity. Also, subjects who are more familiar personally with the target singers have better identification scores than target-unfamiliar subjects on the timbre-converted singing voices.",https://aclanthology.org/2019.rocling-1.28,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2019,October,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),"Kang, Kuan-Yi  and
Liu, Yi-Wen  and
Wang, Hsin-Min",Influences of Prosodic Feature Replacement on the Perceived Singing Voice Identity,,rocling
2016.gwc-1.32,"['Parsing', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']","['Data Preparation', 'Semantic Parsing']",['Semantic Role Labeling'],"The aim of this paper is to show a language-independent process of creating a new semantic relation between adjectives and nouns in wordnets. The existence of such a relation is expected to improve the detection of figurative language and sentiment analysis SA. The proposed method uses an annotated corpus to explore the semantic knowledge contained in linguistic constructs performing as the rhetorical figure Simile. Based on the frequency of occurrence of similes in an annotated corpus, we propose a new relation, which connects the noun synset with the synset of an adjective representing that noun's specific attribute. We elaborate on adding this new relation in the case of the Serbian WordNet SWN. The proposed method is evaluated by human judgement in order to determine the relevance of automatically selected relation items. The evaluation has shown that 84% of the automatically selected and the most frequent linguistic constructs, whose frequency threshold was equal to 3, were also selected by humans.",https://aclanthology.org/2016.gwc-1.32,Global Wordnet Association,2016,27--30 January,Proceedings of the 8th Global WordNet Conference (GWC),"Mladenovi{\'c}, Miljana  and
Mitrovi{\'c}, Jelena  and
Krstev, Cvetana",A Language-independent Model for Introducing a New Semantic Relation Between Adjectives and Nouns in a WordNet,,gwc
2020.nlpcss-1.15,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Supervised Learning', 'Data Preparation']",['Annotation Processes'],"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients' life. To support this task, we present an approach that extracts indications of independence on different life aspects from the day-to-day documentation that social workers create. We describe the process of collecting and annotating a corresponding corpus created from data records of two social work institutions with a focus on disability care. We show that the agreement on the task of annotating the observations of social workers with respect to discrete independent levels yields a high agreement of .74 as measured by Fleiss' Kappa. We present a classification approach towards automatically classifying an observation into the discrete independence levels and present results for different types of classifiers. Against our original expectation, we show that we reach F-Measures macro of 95% averaged across topics, showing that this task can be automatically solved.",https://aclanthology.org/2020.nlpcss-1.15,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Maier, Angelika  and
Cimiano, Philipp",Predicting independent living outcomes from written reports of social workers,10.18653/v1/2020.nlpcss-1.15,nlpcss
2020.nlptea-1.14,"['Error Detection and Correction', 'Model Architectures', 'Low-resource Languages']",['Grammatical Error Correction (GEC)'],,"In the process of learning Chinese, second language learners may have various grammatical errors due to the negative transfer of native language. This paper describes our submission to the NLPTEA 2020 shared task on CGED. We present a hybrid system that utilizes both detection and correction stages. The detection stage is a sequential labelling model based on BiLSTM-CRF and BERT contextual word representation. The correction stage is a hybrid model based on the n-gram and Seq2Seq. Without adding additional features and external data, the BERT contextual word representation can effectively improve the performance metrics of Chinese grammatical error detection and correction.",https://aclanthology.org/2020.nlptea-1.14,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Zan, Hongying  and
Han, Yangchao  and
Huang, Haotian  and
Yan, Yingjie  and
Wang, Yuke  and
Han, Yingjie",Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task,,nlptea
2020.cllrd-1.5,"['Data Management and Generation', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Abstract Meaning Representation (AMR)']",['Annotation Processes'],"Meaning Representations AMRs, a syntax-free representation of phrase semantics Banarescu et al., 2013 , are useful for capturing the meaning of a phrase and reflecting the relationship between concepts that are referred to. However, annotating AMRs is time consuming and expensive. The existing annotation process requires expertly trained workers who have knowledge of an extensive set of guidelines for parsing phrases. In this paper, we propose a cost-saving two-step process for the creation of a corpus of AMR-phrase pairs for spatial referring expressions. The first step uses non-specialists to perform simple annotations that can be leveraged in the second step to accelerate the annotation performed by the experts. We hypothesize that our process will decrease the cost per annotation and improve consistency across annotators. Few corpora of spatial referring expressions exist and the resulting language resource will be valuable for referring expression comprehension and generation modeling.",https://aclanthology.org/2020.cllrd-1.5,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',"Martin, Mary  and
Mauceri, Cecilia  and
Palmer, Martha  and
Heckman, Christoffer",Leveraging Non-Specialists for Accurate and Time Efficient AMR Annotation,,cllrd
2020.nlp4if-1.1,"['Domain-specific NLP', 'Data Management and Generation', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Data Preparation', 'Misinformation Detection', 'Medical and Clinical NLP']","['NLP for Social Media', 'Fake News Detection']","The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact checking algorithm that retrieves the most relevant facts concerning user claims about particular COVID-19 claims. The second model verifies the level of ""truth"" in the claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of models based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.",https://aclanthology.org/2020.nlp4if-1.1,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 3rd NLP4IF Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Vijjali, Rutvik  and
Potluri, Prathyush  and
Kumar, Siddharth  and
Teki, Sundeep",Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking,,nlp4if
2020.sigdial-1.27,"['Audio Generation and Processing', 'Discourse Analysis']",,,"Acoustic/prosodic a/p entrainment has been associated with multiple positive social aspects of human-human conversations. However, research on its effects is still preliminary, first because how to model it is far from standardized, and second because most of the reported findings rely on small corpora or on corpora collected in experimental setups. The present article has a twofold purpose: 1 it proposes a unifying statistical framework for modeling a/p entrainment, and 2 it tests on two large corpora of spontaneous telephone interactions whether three metrics derived from this framework predict positive social aspects of the conversations. The corpora differ in their spoken language, domain, and positive social outcome attached. To our knowledge, this is the first article studying relations between a/p entrainment and positive social outcomes in such large corpora of spontaneous dialog. Our results suggest that our metrics effectively predict, up to some extent, positive social aspects of conversations, which not only validates the methodology, but also provides further insights into the elusive topic of entrainment in human-human conversation.",https://aclanthology.org/2020.sigdial-1.27,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"G{\'a}lvez, Ramiro H.  and
Gauder, Lara  and
Luque, Jordi  and
Gravano, Agust{\'\i}n",A unifying framework for modeling acoustic/prosodic entrainment: definition and evaluation on two large corpora,,sigdial
2020.rdsm-1.4,"['Model Architectures', 'Classification Applications']","['Rumor Detection', 'Stance Detection']",,"Correctly classifying stances of replies can be significantly helpful for the automatic detection and classification of online rumours. One major challenge is that there are considerably more non-relevant replies comments than informative ones supports and denies, making the task highly imbalanced. In this paper we revisit the task of rumour stance classification, aiming to improve the performance over the informative minority classes. We experiment with traditional methods for imbalanced data treatment with feature-and BERT-based classifiers. Our models outperform all systems in RumourEval 2017 shared task and rank second in RumourEval 2019.",https://aclanthology.org/2020.rdsm-1.4,Association for Computational Linguistics,2020,December,Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM),"Li, Yue  and
Scarton, Carolina",Revisiting Rumour Stance Classification: Dealing with Imbalanced Data,,rdsm
2020.msr-1.2,"['Parsing', 'Model Architectures']",['Syntactic Parsing'],,"We present a system for mapping Universal Dependency structures to raw text which learns to restore word order by training an Interpreted Regular Tree Grammar IRTG that establishes a mapping between string and graph operations. The reinflection step is handled by a standard sequence-to-sequence architecture with a biLSTM encoder and an LSTM decoder with attention. We modify our 2019 system Kovács et al., 2019 with a new grammar induction mechanism that allows IRTG rules to operate on lemmata in addition to part-of-speech tags and ensures that each word and its dependents are reordered using the most specific set of learned patterns. We also introduce a hierarchical approach to word order restoration that independently determines the word order of each clause in a sentence before arranging them with respect to the main clause, thereby improving overall readability and also making the IRTG parsing task tractable. We participated in the 2020 Surface Realization Shared task, subtrack T1a shallow, closed. Human evaluation shows we achieve significant improvements on two of the three out-of-domain datasets compared to the 2019 system we modified. Both components of our system are available on GitHub under an MIT license.",https://aclanthology.org/2020.msr-1.2,Association for Computational Linguistics,2020,December,Proceedings of the Third Workshop on Multilingual Surface Realisation,"Recski, G{\'a}bor  and
Kov{\'a}cs, {\'A}d{\'a}m  and
G{\'e}mes, Kinga  and
{\'A}cs, Judit  and
Kornai, Andras",BME-TUW at SR'20: Lexical grammar induction for surface realization,,msr
2020.aacl-main.31,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Humor Detection', 'Hate and Offensive Speech Detection', 'Sentiment Analysis (SA)', 'Sarcasm Detection', 'Multilabel Text Classification']",,"In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and sentiment analysis on a somewhat complicated form of information, i.e., memes. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For multi-tasking, we propose two attention-like mechanisms viz., Inter-task Relationship Module iTRM and Inter-class Relationship Module iCRM. The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, iCRM develops relations between the different classes of tasks. Finally, representations from both the attentions are concatenated and shared across the five tasks i.e., humour, sarcasm, offensive, motivational, and sentiment for multi-tasking. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of memes annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-theart systems Baseline and SemEval 2020 winner. The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.",https://aclanthology.org/2020.aacl-main.31,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Chauhan, Dushyant Singh  and
S R, Dhanush  and
Ekbal, Asif  and
Bhattacharyya, Pushpak","All-in-One: A Deep Attentive Multi-task Learning Framework for Humour, Sarcasm, Offensive, Motivation, and Sentiment on Memes",,aacl
2020.readi-1.1,"['Data Management and Generation', 'Domain-specific NLP', 'Error Detection and Correction', 'Low-resource Languages']","['Data Preparation', 'Medical and Clinical NLP']",,"Spell checkers and other proofreading software are crucial tools for people with dyslexia and other reading disabilities. Most spell checkers automatically detect spelling mistakes by looking up individual words and seeing if they exist in the vocabulary. However, one of the biggest challenges of automatic spelling correction is how to deal with real-word errors, i.e. spelling mistakes which lead to a real but unintended word, such as when then is written in place of than. These errors account for 20% of all spelling mistakes made by people with dyslexia. As both words exist in the vocabulary, a simple dictionary lookup will not detect the mistake. The only way to disambiguate which word was actually intended is to look at the context in which the word appears. This problem is particularly apparent in languages with rich morphology where there is often minimal orthographic difference between grammatical items. In this paper, we present our novel confusion set corpus for Icelandic and discuss how it could be used for context-sensitive spelling correction. We have collected word pairs from seven different categories, chosen for their homophonous properties, along with sentence examples and frequency information from said pairs. We present a small-scale machine learning experiment using a decision tree binary classification which results range from 73% to 86% average accuracy with 10-fold cross validation. While not intended as a finalized result, the method shows potential and will be improved in future research.",https://aclanthology.org/2020.readi-1.1,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Fri{\dh}riksd{\'o}ttir, Steinunn Rut  and
Ingason, Anton Karl",Disambiguating Confusion Sets as an Aid for Dyslexic Spelling,,readi
2020.acl-main.63,"['Dialogue Systems', 'Model Architectures', 'Learning Paradigms']","['Unsupervised Learning', 'Supervised Learning', 'Response Generation', 'Reinforcement Learning']",,"In modular dialogue systems, natural language understanding NLU and natural language generation NLG are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work Su et al., 2019 is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. 1",https://aclanthology.org/2020.acl-main.63,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Su, Shang-Yu  and
Huang, Chao-Wei  and
Chen, Yun-Nung",Towards Unsupervised Language Understanding and Generation by Joint Dual Learning,10.18653/v1/2020.acl-main.63,acl
2020.aespen-1.7,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Entity Linking', 'Event Extraction']",,"Previous efforts to automate the detection of social and political events in text have primarily focused on identifying events described within single sentences or documents. Within a corpus of documents, these automated systems are unable to link event referencesrecognize singular events across multiple sentences or documents. A separate literature in computational linguistics on event coreference resolution attempts to link known events to one another within and across documents. I provide a data set for evaluating methods to identify certain political events in text and to link related texts to one another based on shared events. The data set, Headlines of War, is built on the Militarized Interstate Disputes data set and offers headlines classified by dispute status and headline pairs labeled with coreference indicators. Additionally, I introduce a model capable of accomplishing both tasks. The multi-task convolutional neural network is shown to be capable of recognizing events and event coreferences given the headlines' texts and publication dates.",https://aclanthology.org/2020.aespen-1.7,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"Radford, Benjamin",Seeing the Forest and the Trees: Detection and Cross-Document Coreference Resolution of Militarized Interstate Disputes,,aespen
2020.iwslt-1.32,"['Data Management and Generation', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Data Augmentation']",,"A variety of natural language tasks require processing of textual data which contains a mix of natural language and formal languages such as mathematical expressions. In this paper, we take unit conversions as an example and propose a data augmentation technique which lead to models learning both translation and conversion tasks as well as how to adequately switch between them for end-to-end localization.",https://aclanthology.org/2020.iwslt-1.32,Association for Computational Linguistics,2020,July,Proceedings of the 17th International Conference on Spoken Language Translation,"Dinu, Georgiana  and
Mathur, Prashant  and
Federico, Marcello  and
Lauly, Stanislas  and
Al-Onaizan, Yaser",Joint Translation and Unit Conversion for End-to-end Localization,10.18653/v1/2020.iwslt-1.32,iwslt
2020.parlaclarin-1.4,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"The Parliament of the Czech Republic consists of two chambers: the Chamber of Deputies Lower House and the Senate Upper House. In our work, we focus on agenda and documents that relate to the Chamber of Deputies. Namely, we pay particular attention to stenographic protocols that record the Chamber of Deputies' meetings. Our overall goal is to continually compile the protocols into the TEI encoded corpus ParCzech and make the corpus accessible in a more user friendly way than the Parliament publishes the protocols. In the very first stage of the compilation, the ParCzech corpus consists of the 2013+ protocols that we make accessible and searchable in the TEITOK web-based platform.",https://aclanthology.org/2020.parlaclarin-1.4,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Hladka, Barbora  and
Kopp, Maty{\'a}{\v{s}}  and
Stra{\v{n}}{\'a}k, Pavel",Compiling Czech Parliamentary Stenographic Protocols into a Corpus,,parlaclarin
2020.ecomnlp-1.9,"['Model Architectures', 'Text Generation']",['Transformer Models'],,"E-commerce sites include advertising slogans along with information regarding items. Slogans can attract viewers' attention to increase sales or visits by emphasizing advantages of items. The aim of this study is to generate a slogan from a description of an item. To generate a slogan, we apply an encoder-decoder model which has shown effectiveness in many kinds of natural language generation tasks, such as abstractive summarization. However, slogan generation task has three characteristics that distinguish it from other natural language generation tasks: distinctiveness, topic emphasis, and style difference. To handle these three characteristics, we propose a compressed representation-based reconstruction model with refer-attention and conversion layers. The results of experiments with automatic and human evaluations indicate that our method achieves higher performance than conventional methods.",https://aclanthology.org/2020.ecomnlp-1.9,Association for Computational Linguistics,2020,December,Proceedings of Workshop on Natural Language Processing in E-Commerce,"Misawa, Shotaro  and
Miura, Yasuhide  and
Taniguchi, Tomoki  and
Ohkuma, Tomoko",Distinctive Slogan Generation with Reconstruction,,ecomnlp
2020.bucc-1.7,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['Medical and Clinical NLP', 'Data Analysis']",['Biomedical NLP'],"This paper describes and evaluates three methods for reducing the research space for parallel sentences in monolingual comparable corpora. Basically, when searching for parallel sentences between two comparable documents, all the possible sentence pairs between the documents have to be considered, which introduces a great degree of imbalance between parallel pairs and non-parallel pairs. This is a problem because, even with a highly performing algorithm, a lot of noise will be present in the extracted results, thus introducing a need for an extensive and costly manual check phase. We propose to study how we can drastically reduce the number of sentence pairs that have to be fed to a classifier so that the results can be manually handled. We work on a manually annotated subset obtained from a French comparable corpus.",https://aclanthology.org/2020.bucc-1.7,European Language Resources Association,2020,May,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,"Cardon, R{\'e}mi  and
Grabar, Natalia",Reducing the Search Space for Parallel Sentences in Comparable Corpora,,bucc
2020.nuse-1.8,"['Data Management and Generation', 'Information Extraction', 'Classification Applications']","['Data Preparation', 'Event Extraction']",['Annotation Processes'],"In this paper we introduce the problem of extracting events from dialogue. Previous work on event extraction focused on newswire, however we are interested in extracting events from spoken dialogue. To ground this study, we annotated dialogue transcripts from fourteen episodes of the podcast This American Life. This corpus contains 1,038 utterances, made up of 16,962 tokens, of which 3,664 represent events. The agreement for this corpus has a Cohen's κ of 0.83. We have open sourced this corpus for the NLP community. With this corpus in hand, we trained support vector machines SVM to correctly classify these phenomena with 0.68 F1, when using episodefold cross-validation. This is nearly 100% higher F1 than the baseline classifier. The SVM models achieved performance of over 0.75 F1 on some testing folds. We report the results for SVM classifiers trained with four different types of features verb classes, part of speech tags, named entities, and semantic role labels, and different machine learning protocols under-sampling and trigram context. This work is grounded in narratology and computational models of narrative. It is useful for extracting events, plot, and story content from spoken dialogue.",https://aclanthology.org/2020.nuse-1.8,Association for Computational Linguistics,2020,July,"Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events","Eisenberg, Joshua  and
Sheriff, Michael",Automatic extraction of personal events from dialogue,10.18653/v1/2020.nuse-1.8,nuse
2020.acl-main.533,"['Model Architectures', 'Audio Generation and Processing', 'Embeddings', 'Machine Translation (MT)']","['Word Embeddings', 'Automatic Speech Recognition (ASR)']",,"Speech translation ST aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.",https://aclanthology.org/2020.acl-main.533,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Chuang, Shun-Po  and
Sung, Tzu-Wei  and
Liu, Alexander H.  and
Lee, Hung-yi","Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation",10.18653/v1/2020.acl-main.533,acl
2020.acl-main.67,"['Model Architectures', 'Knowledge Representation and Reasoning', 'Text Generation']","['Data-to-Text Generation', 'Graph Neural Networks (GNNs)', 'Abstract Meaning Representation (AMR)']",,"Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation -A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations AMR. Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1 The message propagation process in AMR graphs is only guided by the firstorder adjacency information. 2 The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higherorder neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.",https://aclanthology.org/2020.acl-main.67,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Zhao, Yanbin  and
Chen, Lu  and
Chen, Zhi  and
Cao, Ruisheng  and
Zhu, Su  and
Yu, Kai",Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks,10.18653/v1/2020.acl-main.67,acl
2016.tc-1.7,"['Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",,,"This paper describes an experiment conducted by the author in November 2015 with 69 MSc Translation students at CenTraS @ UCL covering 14 target languages and in August 2016 with 30 professional translators in Saudi Arabia covering English to Arabic. The experiment was inspired by Lynne Bowker's pilot study Productivity vs Quality? A pilot study on the impact of translation memory systems published in Localisation Focus in March 2005. The author of this paper wanted to find out whether translators who are fairly new to translation technology would ""blindly"" trust the content of a TM or whether they would still check the content thoroughly and make any necessary changes to the translation. Students and professional translators were asked to translate a short text consisting of 14 sentences and a total of 217 words in Wordfast Anywhere/SDL Trados Studio 2015. They also received a translation memory TM for their respective language combination. All TMs contained mistakes, which the author did not mention to the students and the professional translators. Interestingly, while the professional translators fared better at editing fuzzy matches than the students, they did not pick up on incorrect 100% matches as well as the student translators, tended to lack attention to detail by, for example, introducing double spaces into sentences, and not all professional translators translated the new sentences given for translation.",https://aclanthology.org/2016.tc-1.7,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Ford, Daniela",Can you trust a TM? Results of an experiment conducted in November 2015 and August 2016 with students and professional translators,,tc
2020.acl-main.296,"['Information Extraction', 'Classification Applications']",['Sentiment Analysis (SA)'],['Aspect-Based SA (ABSA)'],"Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis ABSA. The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems. However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms. Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs. To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction PAOTE. Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works. We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries. Meanwhile, the pair-wise relations are jointly identified using the span representations. Extensive experiments show that our model consistently outperforms stateof-the-art methods.",https://aclanthology.org/2020.acl-main.296,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Zhao, He  and
Huang, Longtao  and
Zhang, Rong  and
Lu, Quan  and
Xue, Hui",SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction,10.18653/v1/2020.acl-main.296,acl
2020.readi-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Medical and Clinical NLP', 'Text Simplification', 'Data Analysis']",,"The objective of this work is to introduce text simplification as a potential reading aid to help improve the poor reading performance experienced by visually impaired individuals. As a first step, we explore what makes a text especially complex when read with low vision, by assessing the individual effect of three word properties frequency, orthographic similarity and length on reading speed in the presence of Central visual Field Loss CFL. Individuals with bilateral CFL induced by macular diseases read pairs of French sentences displayed with the self-paced reading method. For each sentence pair, sentence n contained a target word matched with a synonym word of the same length included in sentence n+1. Reading time was recorded for each target word. Given the corpus we used, our results show that 1 word frequency has a significant effect on reading time the more frequent the faster the reading speed with larger amplitude in the range of seconds compared to normal vision; 2 word neighborhood size has a significant effect on reading time the more neighbors the slower the reading speed, this effect being rather small in amplitude, but interestingly reversed compared to normal vision; 3 word length has no significant effect on reading time. Supporting the development of new and more effective assistive technology to help low vision is an important and timely issue, with massive potential implications for social and rehabilitation practices. The end goal of this project will be to use our findings to custom text simplification to this specific population and use it as an optimal and efficient reading aid.",https://aclanthology.org/2020.readi-1.5,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Sauvan, Lauren  and
Stolowy, Natacha  and
Aguilar, Carlos  and
Fran{\c{c}}ois, Thomas  and
Gala, N{\'u}ria  and
Matonti, Fr{\'e}d{\'e}ric  and
Castet, Eric  and
Calabr{\`e}se, Aur{\'e}lie",Text Simplification to Help Individuals with Low Vision Read More Fluently,,readi
2020.emnlp-main.239,"['Model Architectures', 'Learning Paradigms']",['Adversarial Learning'],,"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In realworld matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.",https://aclanthology.org/2020.emnlp-main.239,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Yu, Weijie  and
Xu, Chen  and
Xu, Jun  and
Pang, Liang  and
Gao, Xiaopeng  and
Wang, Xiaozhao  and
Wen, Ji-Rong",Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,10.18653/v1/2020.emnlp-main.239,emnlp
2021.argmining-1.9,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Argument Mining', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"Public participation processes allow citizens to engage in municipal decision-making processes by expressing their opinions on specific issues. Municipalities often only have limited resources to analyze a possibly large amount of textual contributions that need to be evaluated in a timely and detailed manner. Automated support for the evaluation is therefore essential, e.g. to analyze arguments. In this paper, we address A the identification of argumentative discourse units and B their classification as major position or premise in German public participation processes. The objective of our work is to make argument mining viable for use in municipalities. We compare different argument mining approaches and develop a generic model that can successfully detect argument structures in different datasets of mobility-related urban planning. We introduce a new data corpus comprising five public participation processes. In our evaluation, we achieve high macro F 1 scores 0.76 -0.80 for the identification of argumentative units; 0.86 -0.93 for their classification on all datasets. Additionally, we improve previous results for the classification of argumentative units on a similar German online participation dataset.",https://aclanthology.org/2021.argmining-1.9,Association for Computational Linguistics,2021,November,Proceedings of the 8th Workshop on Argument Mining,"Romberg, Julia  and
Conrad, Stefan",Citizen Involvement in Urban Planning - How Can Municipalities Be Supported in Evaluating Public Participation Processes for Mobility Transitions?,10.18653/v1/2021.argmining-1.9,argmining
2020.latechclfl-1.8,"['Information Extraction', 'Image and Video Processing', 'Low-resource Languages']",['Optical Character Recognition (OCR)'],,"We present Vital Records, a demonstrator based on deep-learning approaches to handwritten-text recognition, table processing and information extraction, which enables data from century-old documents to be parsed and analysed, making it possible to explore death records in space and time. This demonstrator provides a user interface for browsing and visualising data extracted from 80,000 handwritten pages of tabular data.",https://aclanthology.org/2020.latechclfl-1.8,International Committee on Computational Linguistics,2020,December,"Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Dejean, Herve  and
Meunier, Jean-Luc",Vital Records: Uncover the past from historical handwritten records,,latechclfl
2020.challengehml-1.6,"['Domain-specific NLP', 'Classification Applications', 'Learning Paradigms']",['Multimodal Learning'],,"Behavioral cues play a significant part in human communication and cognitive perception. In most professional domains, employee recruitment policies are framed such that both professional skills and personality traits are adequately assessed. Hiring interviews are structured to evaluate expansively a potential employee's suitability for the position -their professional qualifications, interpersonal skills, ability to perform in critical and stressful situations, in the presence of time and resource constraints, etc. Therefore, candidates need to be aware of their positive and negative attributes and be mindful of behavioral cues that might have adverse effects on their success. We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc. We perform a comprehensive analysis that includes the interviewee's facial expressions, speech, and prosodic information, using the video, audio, and text transcripts obtained from the recorded interview. We use these multimodal data sources to construct a composite representation, which is used for training machine learning classifiers to predict the class labels. Such analysis is then used to provide constructive feedback to the interviewee for their behavioral cues and body language. Experimental validation showed that the proposed methodology achieved promising results.",https://aclanthology.org/2020.challengehml-1.6,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Agrawal, Anumeha  and
Anil George, Rosa  and
Ravi, Selvan Sunitha  and
Kamath S, Sowmya  and
Kumar, Anand",Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback,10.18653/v1/2020.challengehml-1.6,challengehml
2020.nlptea-1.10,"['Data Management and Generation', 'Error Detection and Correction', 'Model Architectures', 'Low-resource Languages']","['Grammatical Error Correction (GEC)', 'Data Augmentation']",,"A better Chinese Grammatical Error Diagnosis CGED system for automatic Grammatical Error Correction GEC can benefit foreign Chinese learners and lower Chinese learning barriers. In this paper, we introduce our solution to the CGED2020 Shared Task Grammatical Error Correction in detail. The task aims to detect and correct grammatical errors that occur in essays written by foreign Chinese learners. Our solution combined data augmentation methods, spelling check methods, and generative grammatical correction methods, and achieved the best recall score in the Top 1 Correction track. Our final result ranked fourth among the participants.",https://aclanthology.org/2020.nlptea-1.10,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Wang, Yi  and
Yuan, Ruibin  and
Luo, Yan{`}gen  and
Qin, Yufang  and
Zhu, NianYong  and
Cheng, Peng  and
Wang, Lihuan",Chinese Grammatical Error Correction Based on Hybrid Models with Data Augmentation,,nlptea
2020.nlptea-1.11,"['Error Detection and Correction', 'Low-resource Languages']",['Grammatical Error Correction (GEC)'],,"In this paper, we introduce our system for NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis CGED. In recent years, pre-trained models have been extensively studied, and several downstream tasks have benefited from their utilization. In this study, we treat the grammar error diagnosis GED task as a grammatical error correction GEC problem and use a method that incorporates a pre-trained model into an encoderdecoder model to solve this problem.",https://aclanthology.org/2020.nlptea-1.11,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Wang, Hongfei  and
Komachi, Mamoru",TMU-NLP System Using BERT-based Pre-trained Model to the NLP-TEA CGED Shared Task 2020,,nlptea
2020.codi-1.5,"['Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Data Preparation', 'Humor Detection']",['Annotation Processes'],"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people's stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality is difficult to achieve. In order to minimise the time and effort needed for producing an excellent script, we explore ways of predicting the audience's response from the comedy scripts. For this task, we present a corpus of annotated scripts from popular television entertainment programmes in recent years. Annotations include a text classification labels, indicating which actor's lines made the studio audience laugh; b information extraction labels, i.e. the text spans that made the audience laughed immediately after the performers said them. The corpus will also be useful for dialogue systems and discourse analysis, since our annotations are based on entire scripts. In addition, we evaluate different baseline algorithms. Experimental results demonstrate that BERT models can achieve the best predictions among all the baseline methods. Furthermore, we conduct an error analysis and investigate predictions across scripts with different styles. 1 * The research was conducted during non-working time. The idea of this research was inspired by a discussion with my friend about an entertainment TV programme in which the comedians mentioned the difficulties of producing a highquality script.",https://aclanthology.org/2020.codi-1.5,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Computational Approaches to Discourse,"Li, Maolin",Supporting Comedy Writers: Predicting Audience's Response from Sketch Comedy and Crosstalk Scripts,10.18653/v1/2020.codi-1.5,codi
2020.wosp-1.6,"['Text Clustering', 'Data Management and Generation', 'Topic Modeling']",['Data Preparation'],,"Mainly due to the open access movement, the number of scholarly papers we can freely access is drastically increasing. A huge amount of papers is a promising resource for text mining and machine learning. Given a set of papers, for example, we can grasp past or current trends in a research community. Compared to the trend detection, it is more difficult to forecast trends in the near future, since the number of occurrences of some features, which are major cues for automatic detection, such as the word frequency, is quite small before such a trend will emerge. As a first step toward trend forecasting, this paper is devoted to finding subtle trends. To do this, the authors propose an index for keywords, called normalized impact index, and visualize keywords and their indices as a heat map. The authors have conducted case studies using some keywords already known as popular, and we found some keywords whose frequencies are not so large but whose indices are large.",https://aclanthology.org/2020.wosp-1.6,Association for Computational Linguistics,2020,05 August,Proceedings of the 8th International Workshop on Mining Scientific Publications,"Ikeda, Daisuke  and
Taniguchi, Yuta  and
Koga, Kazunori",The Normalized Impact Index for Keywords in Scholarly Papers to Detect Subtle Research Topics,,wosp
2019.rocling-1.10,"['Information Retrieval', 'Domain-specific NLP', 'Dialogue Systems', 'Classification Applications']","['Information Filtering', 'Intent Detection', 'Chatbots', 'NLP for Finance']",['Recommender Systems'],"In the insurance industry, lots of effort is putting into helping the customer to solve their problems that occurred during and after purchasing cycle and helping telemarketers to practice selling skills. Chat bots and assistant bots are widely used in these business scenarios, but building a bot application from scratch is expensive. In this paper, a human-machine interaction platform specially designed for intelligent bot applications in insurance industry that combined the technologies of Question Answering (QA), task-oriented dialogue and chit-chat was proposed and we demonstrate the architecture design of this platform, key technologies and the scenario of applications in real-world insurance industry. It has been supporting many intelligent bot applications of insurance industry already, such as Intelligent Coach Bot (ICB) which helps telemarketers to practice their selling skills, Intelligent Customer Service Bot (ICSB) which provides after-sales services and Insurance Advisor Bot (IAB) which helps customer to purchase the most suitable insurance product. Currently, these bot applications serve millions of users per day and are able to solve 80% of the online problems.",https://aclanthology.org/2019.rocling-1.10,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2019,October,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),"Tan, Wei  and
Chang, Chia-Hao  and
Mo, Yang  and
Jiang, Lian-Xin  and
Li, Gen  and
Hou, Xiao-Long  and
Chen, Chu  and
Huang, Yu-Sheng  and
Huang, Meng-Yuan  and
Shen, Jian-Ping",A Real-World Human-Machine Interaction Platform in Insurance Industry,,rocling
2020.aespen-1.11,"['Text Clustering', 'Domain-specific NLP', 'Information Extraction', 'Model Architectures']","['NLP for News and Media', 'Coreference Resolution']",,"This paper summarizes our group's efforts in the event sentence coreference identification shared task, which is organized as part of the Automated Extraction of Socio-Political Events from News AESPEN Workshop. Our main approach consists of three steps. We initially use a transformer based model to predict whether a pair of sentences refer to the same event or not. Later, we use these predictions as the initial scores and recalculate the pair scores by considering the relation of sentences in a pair with respect to other sentences. As the last step, final scores between these sentences are used to construct the clusters, starting with the pairs with the highest scores. Our proposed approach outperforms the baseline approach across all evaluation metrics.",https://aclanthology.org/2020.aespen-1.11,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"{\""O}rs, Faik Kerem  and
Yeniterzi, S{\""u}veyda  and
Yeniterzi, Reyyan",Event Clustering within News Articles,,aespen
2016.lilt-13.1,"['Data Management and Generation', 'Information Extraction']","['Data Preparation', 'Coreference Resolution']",,"Verb phrase VP ellipsis is the omission of a verb phrase whose meaning can be reconstructed from the linguistic or real-world context. It is licensed in English by auxiliary verbs, often modal auxiliaries: She can go to Hawaii but he can't e. This paper describes a system called ViPER VP Ellipsis Resolver that detects and resolves VP ellipsis, relying on linguistic principles such as syntactic parallelism, modality correlations, and the delineation of core vs. peripheral sentence constituents. The key insight guiding the work is that not all cases of ellipsis are equally difficult: some can be detected and resolved with high confidence even before we are able to build systems with human-level semantic and pragmatic understanding of text.",https://aclanthology.org/2016.lilt-13.1,CSLI Publications,2016,,"Linguistic Issues in Language Technology, Volume 13, 2016","McShane, Marjorie  and
Babkin, Petr",Detection and Resolution of Verb Phrase Ellipsis,,lilt
2020.rail-1.5,"['Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"In this paper, we compare four state-of-the-art neural network dependency parsers for the Semitic language Amharic. As Amharic is a morphologically-rich and less-resourced language, the out-of-vocabulary OOV problem will be higher when we develop data-driven models. This fact limits researchers to develop neural network parsers because the neural network requires large quantities of data to train a model. We empirically evaluate neural network parsers when a small Amharic treebank is used for training. Based on our experiment, we obtain an 83.79 LAS score using the UDPipe system. Better accuracy is achieved when the neural parsing system uses external resources like word embedding. Using such resources, the LAS score for UDPipe improves to 85.26. Our experiment shows that the neural networks can learn dependency relations better from limited data while segmentation and POS tagging require much data.",https://aclanthology.org/2020.rail-1.5,European Language Resources Association (ELRA),2020,May,Proceedings of the first workshop on Resources for African Indigenous Languages,"Seyoum, Binyam Ephrem  and
Miyao, Yusuke  and
Mekonnen, Baye Yimam",Comparing Neural Network Parsers for a Less-resourced and Morphologically-rich Language: Amharic Dependency Parser,,rail
2020.computerm-1.2,"['Data Management and Generation', 'Domain-specific NLP', 'Information Extraction', 'Low-resource Languages']",['NLP for Finance'],,"Automatic term extraction ATE from texts is critical for effective terminology work in small speech communities. We present TermPortal, a workbench for terminology work in Iceland, featuring the first ATE system for Icelandic. The tool facilitates standardization in terminology work in Iceland, as it exports data in standard formats in order to streamline gathering and distribution of the material. In the project we focus on the domain of finance in order to do be able to fulfill the needs of an important and large field. We present a comprehensive survey amongst the most prominent organizations in that field, the results of which emphasize the need for a good, up-to-date and accessible termbank and the willingness to use terms in Icelandic. Furthermore we present the ATE tool for Icelandic, which uses a variety of methods and shows great potential with a recall rate of up to 95% and a high C-value, indicating that it competently finds term candidates that are important to the input text.",https://aclanthology.org/2020.computerm-1.2,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Steingr{\'\i}msson, Stein{\th}{\'o}r  and
{\TH}orbergsd{\'o}ttir, {\'A}g{\'u}sta  and
Danielsson, Hjalti  and
Ornolfsson, Gunnar Thor",TermPortal: A Workbench for Automatic Term Extraction from Icelandic Texts,,computerm
2019.lilt-18.2,"['Data Management and Generation', 'Low-resource Languages', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Abstract Meaning Representation (AMR)']",['Annotation Processes'],"Meaning Representation AMR is a meaning representation framework in which the meaning of a full sentence is represented as a single-rooted, acyclic, directed graph. In this article, we describe an on-going project to build a Chinese AMR CAMR corpus, which currently includes 10,149 sentences from the newsgroup and weblog portion of the Chinese TreeBank CTB. We describe the annotation specifications for the CAMR corpus, which follow the annotation principles of English AMR but make adaptations where needed to accommodate the linguistic facts of Chinese. The CAMR specifications also include a systematic treatment of sentence-internal discourse relations. One significant change we have made to the AMR annotation methodology is the inclusion of the alignment between word tokens in the sentence and the concepts/relations in the CAMR annotation to make it easier for automatic parsers to model the correspondence between a sentence and its meaning representation. We develop an annotation tool for CAMR, and the inter-agreement as measured by the Smatch score between the two annotators is 0.83, indicating reliable annotation. We also present some quantitative analysis of the CAMR corpus. 46.71% of the AMRs of the sentences are non-tree graphs. Moreover, the AMR of 88.95% of the sentences has concepts inferred from the context of the sentence but do not correspond to a specific word 2 / LiLT volume 18, issue 1 June 2019 or phrase in a sentence, and the average number of such inferred concepts per sentence is 2.88. These statistics will have to be taken into account when developing automatic Chinese AMR parsers.",https://aclanthology.org/2019.lilt-18.2,CSLI Publications,2019,July,"Linguistic Issues in Language Technology, Volume 18, 2019 - Exploiting Parsed Corpora: Applications in Research, Pedagogy, and Processing","Li, Bin  and
Wen, Yuan  and
Song, Li  and
Qu, Weiguang  and
Xue, Nianwen",Building a Chinese AMR Bank with Concept and Relation Alignments,,lilt
2016.tc-1.6,['Data Management and Generation'],,,"This is to inform the business and decision making communities among the ASLING audience about the high level benefits of bitext and XLIFF 2. Translator and Engineering communities will also benefit, as they need the high level arguments to make the call for XLIFF 2 adoption in their organizations. We start with a conceptual outline what bitext is, what different sorts of bitext exist and how they are useful at various stages in various industry processes, such as translation, localisation, terminology management, quality and sanity assurance projects etc. Examples of projects NOT based on bitext are given, benefits and drawbacks compared on a practical level of tasks performed. The following is demonstrated: That bitext management is a core process for efficient multilingual content value chains; That usage of an open standard bitext creates a greater sum of good than usage of proprietary bitext formats; and finally: That XLIFF 2 is the core format and data model to base bitext management on.",https://aclanthology.org/2016.tc-1.6,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Filip, David",Why XLIFF and why XLIFF 2?,,tc
2020.parlaclarin-1.1,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"This short paper presents the current as of February 2020 state of preparation of the Polish Parliamentary Corpus PPC -an extensive collection of transcripts of Polish parliamentary proceedings dating from 1919 to present. The most evident developments as compared to the 2018 version is harmonization of metadata, standardization of document identifiers, uploading contents of all documents and metadata to the database to enable easier modification, maintenance and future development of the corpus, linking utterances to the political ontology, linking corpus texts to source data and processing historical documents.",https://aclanthology.org/2020.parlaclarin-1.1,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Ogrodniczuk, Maciej  and
Nito{\'n}, Bart{\l}omiej",New Developments in the Polish Parliamentary Corpus,,parlaclarin
2020.lt4hala-1.15,"['Data Management and Generation', 'Multilingual NLP', 'Classification Applications', 'Low-resource Languages']",['Data Preparation'],,"Fictional prose can be broadly divided into narrative and discursive forms with direct speech being central to any discourse representation alongside indirect reported speech and free indirect discourse. This distinction is crucial in digital literary studies and enables interesting forms of narratological or stylistic analysis. The difficulty of automatically detecting direct speech, however, is currently under-estimated. Rule-based systems that work reasonably well for modern languages struggle with the lack of typographical conventions in 19th-century literature. While machine learning approaches to sequence modeling can be applied to solve the task, they typically face a severed skewness in the availability of training material, especially for lesser resourced languages. In this paper, we report the result of a multilingual approach to direct speech detection in a diverse corpus of 19th-century fiction in 9 European languages. The proposed method fine-tunes a transformer architecture with multilingual sentence embedder on a minimal amount of annotated training in each language, and improves performance across languages with ambiguous direct speech marking, in comparison to a carefully constructed regular expression baseline.",https://aclanthology.org/2020.lt4hala-1.15,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Byszuk, Joanna  and
Wo{\'z}niak, Micha{\l}  and
Kestemont, Mike  and
Le{\'s}niak, Albert  and
{\L}ukasik, Wojciech  and
{\v{S}}e{\c{l}}a, Artjoms  and
Eder, Maciej",Detecting Direct Speech in Multilingual Collection of 19th-century Novels,,lt4hala
2020.aespen-1.2,"['Ethics', 'Data Management and Generation']",,,"Not all conflict datasets offer equal levels of coverage, depth, use-ability, and content. A review of the inclusion criteria, methodology, and sourcing of leading publicly available conflict datasets demonstrates that there are significant discrepancies in the output produced by ostensibly similar projects. This keynote will question the presumption of substantial overlap between datasets, and identify a number of important gaps left by deficiencies across core criteria for effective conflict data collection and analysis, including:",https://aclanthology.org/2020.aespen-1.2,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"Raleigh, Clionadh",Keynote Abstract: Too soon? The limitations of AI for event data,,aespen
2020.clinicalnlp-1.27,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Data Preparation', 'Data Augmentation', 'Medical and Clinical NLP']",,"Eligibility criteria in the clinical trials specify the characteristics that a patient must or must not possess in order to be treated according to a standard clinical care guideline. As the process of manual eligibility determination is time-consuming, automatic structuring of the eligibility criteria into various semantic categories or aspects is the need of the hour. Existing methods use hand-crafted rules and feature-based statistical machine learning methods to dynamically induce semantic aspects. However, in order to deal with paucity of aspect-annotated clinical trials data, we propose a novel weakly-supervised co-training based method which can exploit a large pool of unlabeled criteria sentences to augment the limited supervised training data, and consequently enhance the performance. Experiments with 0.2M criteria sentences show that the proposed approach outperforms the competitive supervised baselines by 12% in terms of micro-averaged F1 score for all the aspects. Probing deeper into analysis, we observe domain-specific information boosts up the performance by a significant margin.",https://aclanthology.org/2020.clinicalnlp-1.27,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Dasgupta, Tirthankar  and
Mondal, Ishani  and
Naskar, Abir  and
Dey, Lipika",Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria,10.18653/v1/2020.clinicalnlp-1.27,clinicalnlp
2020.lrec-1.853,"['Error Detection and Correction', 'Classification Applications', 'Low-resource Languages']",,,"In this work we address the processing of negation in Spanish. We first present a machine learning system that processes negation in Spanish. Specifically, we focus on two tasks: i negation cue detection and ii scope identification. The corpus used in the experimental framework is the SFU Review SP -NEG. The results for cue detection outperform state-of-the-art results, whereas for scope detection this is the first system that performs the task for Spanish. Moreover, we provide a qualitative error analysis aimed at understanding the limitations of the system and showing which negation cues and scopes are straightforward to predict automatically, and which ones are challenging.",https://aclanthology.org/2020.lrec-1.853,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Jim{\'e}nez-Zafra, Salud Mar{\'\i}a  and
Morante, Roser  and
Blanco, Eduardo  and
Mart{\'\i}n Valdivia, Mar{\'\i}a Teresa  and
Ure{\~n}a L{\'o}pez, L. Alfonso",Detecting Negation Cues and Scopes in Spanish,,lrec
2020.nlpbt-1.4,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation', 'Learning Paradigms', 'Image and Video Processing']","['Unsupervised Learning', 'Supervised Learning', 'Data Preparation', 'Multimodal Learning']",,"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in structured form.",https://aclanthology.org/2020.nlpbt-1.4,Association for Computational Linguistics,2020,November,Proceedings of the First International Workshop on Natural Language Processing Beyond Text,"Xu, Frank F.  and
Ji, Lei  and
Shi, Botian  and
Du, Junyi  and
Neubig, Graham  and
Bisk, Yonatan  and
Duan, Nan",A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos,10.18653/v1/2020.nlpbt-1.4,nlpbt
2020.lrec-1.34,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"We introduce in this paper a generic approach to combine implicit crowdsourcing and language learning in order to mass-produce language resources LRs for any language for which a crowd of language learners can be involved. We present the approach by explaining its core paradigm that consists in pairing specific types of LRs with specific exercises, by detailing both its strengths and challenges, and by discussing how much these challenges have been addressed at present. Accordingly, we also report on on-going proof-of-concept efforts aiming at developing the first prototypical implementation of the approach in order to correct and extend an LR called ConceptNet based on the input crowdsourced from language learners. We then present an international network called the European Network for Combining Language Learning with Crowdsourcing Techniques enetCollect that provides the context to accelerate the implementation of the generic approach. Finally, we exemplify how it can be used in several language learning scenarios to produce a multitude of NLP resources and how it can therefore alleviate the long-standing NLP issue of the lack of LRs.",https://aclanthology.org/2020.lrec-1.34,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Nicolas, Lionel  and
Lyding, Verena  and
Borg, Claudia  and
Forascu, Corina  and
Fort, Kar{\""e}n  and
Zdravkova, Katerina  and
Kosem, Iztok  and
{\v{C}}ibej, Jaka  and
Arhar Holdt, {\v{S}}pela  and
Millour, Alice  and
K{\""o}nig, Alexander  and
Rodosthenous, Christos  and
Sangati, Federico  and
ul Hassan, Umair  and
Katinskaia, Anisia  and
Barreiro, Anabela  and
Aparaschivei, Lavinia  and
HaCohen-Kerner, Yaakov",Creating Expert Knowledge by Relying on Language Learners: a Generic Approach for Mass-Producing Language Resources by Combining Implicit Crowdsourcing and Language Learning,,lrec
2020.loresmt-1.7,"['Learning Paradigms', 'Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",['Neural MT (NMT)'],,"Standard neural machine translation NMT allows a model to perform translation between a pair of languages. Multilingual neural machine translation NMT, on the other hand, allows a model to perform translation between several language pairs, even between language pairs for which no sentences pair has been seen during training zero-shot translation. This paper presents experiments with zero-shot translation on low resource Indian languages with a very small amount of data for each language pair. We first report results on balanced data over all considered language pairs. We then expand our experiments for additional three rounds by increasing the training data with 2,000 sentence pairs in each round for some of the language pairs. We obtain an increase in translation accuracy with its balanced data settings score multiplied by 7 for Manipuri to Hindi during Round-III of zeroshot translation.",https://aclanthology.org/2020.loresmt-1.7,Association for Computational Linguistics,2020,December,Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages,"Huidrom, Rudali  and
Lepage, Yves",Zero-shot translation among Indian languages,,loresmt
2016.amta-users.14,"['Text Preprocessing', 'Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",['Text Segmentation'],,"In this paper we discuss the motivation and process for planning, building, and monitoring a translation memory TM that serves both human-and machine-translated text segments in a production e-commerce environment. We consider the quality improvements associated with serving human translations for commonly used and mis-translated strings, and the cost benefits of avoiding multiple re-translations of the same source text segments. We cover the technical considerations and architecture for each stage of the TM pipeline, and review the results of using and monitoring the TM in a production setting.",https://aclanthology.org/2016.amta-users.14,The Association for Machine Translation in the Americas,2016,October 28 - November 1,Conferences of the Association for Machine Translation in the Americas: MT Users' Track,"Gillespie, Duncan  and
Russell, Benjamin",Building a Translation Memory to Improve Machine Translation Coverage and Quality,,amta
2020.blackboxnlp-1.24,"['Model Architectures', 'Classification Applications']","['Transformer Models', 'Sentiment Analysis (SA)']",,"Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing NLP models. Very recent work suggests that the self-attention in the Transformer encodes syntactic information; Here, we show that self-attention scores encode semantics by considering sentiment analysis tasks. In contrast to gradient-based feature attribution methods, we propose a simple and effective Layer-wise Attention Tracing LAT method to analyze structured attention weights. We apply our method to Transformer models trained on two tasks that have surface dissimilarities, but share common semanticssentiment analysis of movie reviews and timeseries valence prediction in life story narratives. Across both tasks, words with high aggregated attention weights were rich in emotional semantics, as quantitatively validated by an emotion lexicon labeled by human annotators. Our results show that structured attention weights encode rich semantics in sentiment analysis, and match human interpretations of semantics.",https://aclanthology.org/2020.blackboxnlp-1.24,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Wu, Zhengxuan  and
Nguyen, Thanh-Son  and
Ong, Desmond",Structured Self-AttentionWeights Encode Semantics in Sentiment Analysis,10.18653/v1/2020.blackboxnlp-1.24,blackboxnlp
2020.clinicalnlp-1.25,"['Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP']",,"Domain pretraining followed by task finetuning has become the standard paradigm for NLP tasks, but requires in-domain labelled data for task fine-tuning. To overcome this, we propose to utilise unlabelled domain data by assigning pseudo-labels from a general model. We evaluate the approach on two clinical STS datasets, and achieve r = 0.80 on N2C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA.",https://aclanthology.org/2020.clinicalnlp-1.25,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Wang, Yuxia  and
Verspoor, Karin  and
Baldwin, Timothy",Learning from Unlabelled Data for Clinical Semantic Textual Similarity,10.18653/v1/2020.clinicalnlp-1.25,clinicalnlp
2020.cogalex-1.10,"['Data Management and Generation', 'Information Extraction', 'Classification Applications']",['Data Analysis'],,"Textual definitions constitute a fundamental source of knowledge when seeking the meaning of words, and they are the cornerstone of lexical resources like glossaries, dictionaries, encyclopedia or thesauri. In this paper, we present an in-depth analytical study on the main features relevant to the task of definition extraction. Our main goal is to study whether linguistic structures from canonical the Aristotelian or genus et differentia model can be leveraged to retrieve definitions from corpora in different domains of knowledge and textual genres alike. To this end, we develop a simple linear classifier and analyze the contribution of several sets of linguistic features. Finally, as a result of our experiments, we also shed light on the particularities of existing benchmarks as well as the most challenging aspects of the task.",https://aclanthology.org/2020.cogalex-1.10,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Roig Mirapeix, Mireia  and
Espinosa Anke, Luis  and
Camacho-Collados, Jose",Definition Extraction Feature Analysis: From Canonical to Naturally-Occurring Definitions,,cogalex
2020.splu-1.2,"['Information Extraction', 'Model Architectures']","['Relation Extraction', 'Transformer Models']",,"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT Devlin et al., 2018, which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT Wu and He, 2019 for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model Nichols and Botros, 2015.",https://aclanthology.org/2020.splu-1.2,Association for Computational Linguistics,2020,November,Proceedings of the Third International Workshop on Spatial Language Understanding,"Shin, Hyeong Jin  and
Park, Jeong Yeon  and
Yuk, Dae Bum  and
Lee, Jae Sung",BERT-based Spatial Information Extraction,10.18653/v1/2020.splu-1.2,splu
2020.pam-1.9,['Discourse Analysis'],,,"Judgements about communicative agents evolve over the course of interactions both in how individuals are judged for testimonial reliability and for ideological trustworthiness. This paper combines a theory of social meaning and persona with a theory of reliability within a game-theoretic view of communication, giving a formal model involving interactional histories, repeated game models and ways of evaluating social meaning and trustworthiness.",https://aclanthology.org/2020.pam-1.9,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"McCready, Elin  and
Henderson, Robert",Social Meaning in Repeated Interactions,,pam
2021.dialdoc-1.4,"['Question Answering (QA)', 'Domain-specific NLP', 'Low-resource Languages']",,,"This paper presents a learning assistant that tests one's knowledge and gives feedback that helps a person learn at a faster pace. A learning assistant based on an automated question generation has extensive uses in education, information websites, self-assessment, FAQs, testing ML agents, research, etc. Multiple researchers, and companies have worked on Virtual Assistance, but majorly in English. We",https://aclanthology.org/2021.dialdoc-1.4,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021),"Bommadi, Meghana  and
Terupally, Shreya  and
Mamidi, Radhika",Automatic Learning Assistant in Telugu,10.18653/v1/2021.dialdoc-1.4,dialdoc
2020.bea-1.11,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages']","['Supervised Learning', 'Data Preparation']",['Annotation Processes'],"In undergraduate theses, a good methodology section should describe the series of steps that were followed in performing the research. To assist students in this task, we develop machine-learning models and an app that uses them to provide feedback while students write. We construct an annotated corpus that identifies sentences representing methodological steps and labels when a methodology contains a logical sequence of such steps. We train machine-learning models based on language modeling and lexical features that can identify sentences representing methodological steps with 0.939 f-measure, and identify methodology sections containing a logical sequence of steps with an accuracy of 87%. We incorporate these models into a Microsoft Office Addin, and show that students who improved their methodologies according to the model feedback received better grades on their methodologies.",https://aclanthology.org/2020.bea-1.11,Association for Computational Linguistics,2020,July,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,"Gonz{\'a}lez-L{\'o}pez, Samuel  and
Bethard, Steven  and
Lopez-Lopez, Aurelio",Assisting Undergraduate Students in Writing Spanish Methodology Sections,10.18653/v1/2020.bea-1.11,bea
2020.emnlp-main.400,"['Question Answering (QA)', 'Information Extraction', 'Model Architectures', 'Knowledge Representation and Reasoning']","['Named Entity Recognition (NER)', 'Open-Domain QA', 'Transformer Models']",,"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model-Entities as Experts EAEthat can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as ""Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?"", outperforming an encodergenerator Transformer model with 10× the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance.",https://aclanthology.org/2020.emnlp-main.400,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"F{\'e}vry, Thibault  and
Baldini Soares, Livio  and
FitzGerald, Nicholas  and
Choi, Eunsol  and
Kwiatkowski, Tom",Entities as Experts: Sparse Memory Access with Entity Supervision,10.18653/v1/2020.emnlp-main.400,emnlp
2020.nlp4musa-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Data Analysis']",,"Podcasts are an easily accessible medium of entertainment and information, often covering content from a variety of domains. However, only a few of them garner enough attention to be deemed 'popular'. In this work, we investigate the textual cues that assist in differing popular podcasts from unpopular ones. Despite having very similar polarity and subjectivity, the lexical cues contained in the podcasts are significantly different. Thus, we employ a triplet-based training method, to learn a text-based representation of a podcast, which is then used for a downstream task of ""popularity prediction"". Our best model received an F1 score of 0.82, achieving a relative improvement over the best baseline by 12.3%. * *Equal contribution. Ordered randomly.",https://aclanthology.org/2020.nlp4musa-1.3,Association for Computational Linguistics,2020,16 October,Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA),"Joshi, Brihi  and
Mittal, Shravika  and
Chetan, Aditya",Did You ``Read'' the Next Episode? Using Textual Cues for Predicting Podcast Popularity,,nlp4musa
2020.conll-1.19,"['Data Management and Generation', 'Error Detection and Correction', 'Low-resource Languages', 'Machine Translation (MT)']",['Data Preparation'],,"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions about 70% is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect incomprehensible inadequate and discarded correct incomprehensible adequate translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations.",https://aclanthology.org/2020.conll-1.19,Association for Computational Linguistics,2020,November,Proceedings of the 24th Conference on Computational Natural Language Learning,"Popovi{\'c}, Maja",Relations between comprehensibility and adequacy errors in machine translation output,10.18653/v1/2020.conll-1.19,conll
2020.findings-emnlp.232,['Model Architectures'],['Transformer Models'],,"We present BlockBERT, a lightweight and efficient BERT model for better modeling longdistance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short-or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.",https://aclanthology.org/2020.findings-emnlp.232,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Qiu, Jiezhong  and
Ma, Hao  and
Levy, Omer  and
Yih, Wen-tau  and
Wang, Sinong  and
Tang, Jie",Blockwise Self-Attention for Long Document Understanding,10.18653/v1/2020.findings-emnlp.232,findings
2020.eval4nlp-1.6,"['Evaluation Techniques', 'Embeddings', 'Text Generation']",['Word Embeddings'],,"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time, it has been argued that contextualized word representations exhibit sub-optimal statistical properties for encoding the true similarity between words or sentences. In this paper, we present two techniques for improving encoding representations for similarity metrics: a batch-mean centering strategy that improves statistical properties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERTbackbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks.",https://aclanthology.org/2020.eval4nlp-1.6,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems,"Chen, Xi  and
Ding, Nan  and
Levinboim, Tomer  and
Soricut, Radu",Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance,10.18653/v1/2020.eval4nlp-1.6,eval4nlp
